{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random as rn\n",
    "import pandas as pd\n",
    "from library import Map, GradientDescent\n",
    "import time\n",
    "from random import randrange\n",
    "\n",
    "#profiling libraries\n",
    "import cProfile\n",
    "import pstats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''REGRESSION'''\n",
    "\n",
    "'''Downloading dataset'''\n",
    "dataset = pd.read_csv(\"/Users/alessandrococcia/Downloads/ObesityDataSet.csv\")\n",
    "\n",
    "'''Dataset sampling'''\n",
    "dataset = dataset.sample(frac=1, ignore_index=True) #shuffle sample in the training set\n",
    "\n",
    "'''mapping delle stringhe'''\n",
    "dataset = pd.get_dummies(dataset, drop_first=True, dtype=float)\n",
    "\n",
    "'''normalization'''\n",
    "dataset = (dataset-dataset.mean())/dataset.std()\n",
    "\n",
    "'''Inserimento colonna di bias'''\n",
    "dataset.insert(0, \"Bias\", np.ones(len(dataset)), True) #Bias row\n",
    "\n",
    "\n",
    "TRAIN_TEST_SPLIT_PERCENTAGE = 0.9\n",
    "dataset_training = dataset[:int(len(dataset) * TRAIN_TEST_SPLIT_PERCENTAGE)]\n",
    "dataset_test = dataset[int(len(dataset) * TRAIN_TEST_SPLIT_PERCENTAGE):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yq/gyf11wn125qgkzlcv24mbp3w0000gn/T/ipykernel_67114/3248209574.py:11: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  dataset.replace(to_replace=(\"Obesity_Type_I\", \"Obesity_Type_II\", \"Obesity_Type_III\"), value=1, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "'''CLASSIFICATION'''\n",
    "\n",
    "'''Downloading dataset'''\n",
    "dataset = pd.read_csv(\"/Users/alessandrococcia/Downloads/ObesityDataSet.csv\")\n",
    "\n",
    "'''Dataset sampling'''\n",
    "dataset = dataset.sample(frac=1, ignore_index=True) #shuffle sample in the training set\n",
    "\n",
    "'''mapping delle stringhe'''\n",
    "dataset.replace(to_replace=(\"Insufficient_Weight\", \"Normal_Weight\", \"Overweight_Level_I\", \"Overweight_Level_II\"), value=0, inplace=True)\n",
    "dataset.replace(to_replace=(\"Obesity_Type_I\", \"Obesity_Type_II\", \"Obesity_Type_III\"), value=1, inplace=True)\n",
    "dataset = pd.get_dummies(dataset, drop_first=True, dtype=float)\n",
    "\n",
    "'''normalization'''\n",
    "dataset = (dataset-dataset.mean())/dataset.std()\n",
    "\n",
    "'''Inserimento colonna di bias'''\n",
    "dataset.insert(0, \"Bias\", np.ones(len(dataset)), True) #Bias row\n",
    "\n",
    "\n",
    "TRAIN_TEST_SPLIT_PERCENTAGE = 0.9\n",
    "dataset_training = dataset[:int(len(dataset) * TRAIN_TEST_SPLIT_PERCENTAGE)]\n",
    "dataset_test = dataset[int(len(dataset) * TRAIN_TEST_SPLIT_PERCENTAGE):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoca numero 0\n",
      "Mean Squared Error: 0.05119007048411901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but MLPRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "'''CLASSIFICATION'''\n",
    "\n",
    "output_string = 'NObeyesdad'\n",
    "input_totale = dataset_training.loc[:len(dataset_training), dataset_training.columns != output_string]\n",
    "output_totale = dataset_training.loc[:len(dataset_training), dataset_training.columns == output_string]\n",
    "\n",
    "'''Values to determine threshold'''\n",
    "positive_value, negative_value = max(output_totale['NObeyesdad'].unique()), min(output_totale['NObeyesdad'].unique())\n",
    "threshold = 0.5\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, learning_rate, len_input, num_layers, num_layer_nodes, epochs=1):\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.len_input = len_input\n",
    "        self.num_layers = num_layers\n",
    "        self.num_layer_nodes = num_layer_nodes\n",
    "        self.weight_list = []\n",
    "        self.layer_list = []\n",
    "        self.derivate_list = []\n",
    "        self.threshold = 0.5\n",
    "        self.totalerror = 0\n",
    "\n",
    "\n",
    "        #inizializzazione matrice dei pesi\n",
    "        for i in range(self.num_layers-1):\n",
    "            new_matrix = np.random.rand(self.num_layer_nodes[i], self.num_layer_nodes[i+1]) #matrice con righe=i e colonne i+1: (input,output) di un layer\n",
    "            self.weight_list.append(new_matrix)\n",
    "\n",
    "        #inizializzazione layer\n",
    "        for i in range(self.num_layers):\n",
    "            self.layer_list.append(np.zeros(self.num_layer_nodes[i]))\n",
    "\n",
    "        #inizializzazione lista derivate parziali\n",
    "        for i in range(self.num_layers-1): #non viene contato il layer di input\n",
    "            self.derivate_list.append(np.zeros(self.num_layer_nodes[i+1]))\n",
    "\n",
    "    def activation(self, x, type='sigmoid'): \n",
    "        if type=='sigmoid':\n",
    "            return 1/(1+np.exp(-x))\n",
    "        elif type=='tanh':\n",
    "            return (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))\n",
    "        return None\n",
    "\n",
    "    def activation_derivative(self, x, type='sigmoid'):\n",
    "        if type=='sigmoid':\n",
    "            return (1-self.activation(x, type='sigmoid'))*self.activation(x, type='sigmoid')\n",
    "        return None\n",
    "    \n",
    "    def training(self, dataset_training):\n",
    "        for e in range(self.epochs):\n",
    "            print(\"Epoca numero\", e)\n",
    "            for i in range(dataset_training.shape[0]): #ciclo che itera per ogni sample del dataset \n",
    "                sample = dataset_training.loc[i:i]\n",
    "                self.feedforwarding(sample)\n",
    "                #print(\"Error: \", self.totalerror)\n",
    "                self.backpropagation(i)\n",
    "\n",
    "\n",
    "    def testing(self, dataset_testing) -> list: \n",
    "        predicted_out = []\n",
    "        for i in range(dataset_testing.shape[0]):\n",
    "            self.feedforwarding(dataset_testing.loc[i:i])\n",
    "            predicted_out.append(n.layer_list[-1][0][0])\n",
    "\n",
    "        return predicted_out\n",
    "\n",
    "    def feedforwarding(self, sample):\n",
    "        '''fase di feedforwarding'''\n",
    "        self.layer_list[0] = np.array(sample) #inizializzo l'input della rete ai valori del sample\n",
    "        for layer_index in range(self.num_layers-1): # per ogni weight_matrix esistente\n",
    "            out_unactivated = np.dot(self.layer_list[layer_index], self.weight_list[layer_index])#+0.25 #+ bias test\n",
    "            #print(out_unactivated)\n",
    "            self.layer_list[layer_index+1] = self.activation(out_unactivated)\n",
    "            #print(self.layer_list)\n",
    "\n",
    "    def backpropagation(self, i): \n",
    "        '''fase di backtracking'''\n",
    "        \n",
    "        #inizializzazione errore output\n",
    "        error = -(self.activation(output_totale.loc[i:i])-self.layer_list[-1])\n",
    "        self.derivate_list[-1] = error\n",
    "        for j in range(num_layers-2, 0, -1): #3-2 layer: si parte dall'indice 1 con 0 escluso (es.)\n",
    "            \n",
    "            ad = self.activation_derivative(self.layer_list[j])\n",
    "            summatory = np.matmul(self.weight_list[j], self.derivate_list[j])\n",
    "            self.derivate_list[j-1] = np.multiply(ad, summatory.T).T\n",
    "            '''aggiornamento dei pesi'''\n",
    "            self.weight_list[j] -= self.learning_rate* np.outer(self.layer_list[j], self.derivate_list[j])\n",
    "\n",
    "        #manca l'aggiornamento del peso all'indice 0 che non Ã¨ calcolato nell'ultimo ciclo\n",
    "        self.weight_list[0] -= learning_rate*np.outer(self.layer_list[0], self.derivate_list[0])\n",
    "\n",
    "    def binary_cross_entropy(y_true, y_pred):\n",
    "        epsilon = 1e-15  # Piccola costante per evitare divisioni per zero\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)  # Clip dei valori per evitare valori di logaritmo negativi\n",
    "        return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "        # Calcolo dell'errore per un singolo esempio di training o testing\n",
    "        error = binary_cross_entropy(y_true, y_pred)\n",
    "\n",
    "epochs = 1\n",
    "learning_rate = 1\n",
    "len_input = len(input_totale.columns)\n",
    "num_layer_nodes = [len_input, 100, 1] #numero nodi di ciascun layer da sinistra verso destra\n",
    "num_layers = len(num_layer_nodes) #numero di layer compresi nella rete neurale input e output compresi\n",
    "\n",
    "n = NeuralNetwork(learning_rate=learning_rate, len_input=len_input, \n",
    "                num_layers=num_layers, num_layer_nodes=num_layer_nodes, epochs=epochs\n",
    "                )\n",
    "n.training(input_totale)\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "'''fase di testing'''\n",
    "\n",
    "dataset_test = dataset_test.reset_index(drop=True)\n",
    "X_test = dataset_test.loc[:, dataset_test.columns != output_string]\n",
    "\n",
    "\n",
    "'''fase di testing'''\n",
    "predicted_out = n.testing(X_test)\n",
    "Y_effettivo = np.array(dataset_test[output_string])\n",
    "real_out = np.array(1/(1+np.exp(-Y_effettivo)))\n",
    "\n",
    "\n",
    "# Crea un'istanza di LabelEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Converti le etichette di classe continue in etichette di classe discrete\n",
    "output_totale_discrete = label_encoder.fit_transform(output_totale.values.ravel())\n",
    "\n",
    "# Definisci e addestra il modello di rete neurale\n",
    "model = MLPRegressor(hidden_layer_sizes=(100,100,), activation='logistic', solver='sgd', random_state=42)\n",
    "model.fit(np.array(input_totale), np.array(output_totale_discrete))\n",
    "\n",
    "# Esegui le previsioni sul test set\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "mse = np.mean((real_out-predictions)**2)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoca numero 0\tMSE_training:  5.968733783891043\tMSE_testing:  1.0825818042267148\n",
      "Epoca numero 1\tMSE_training:  1.0106225383484244\tMSE_testing:  1.0821764486239007\n",
      "Epoca numero 2\tMSE_training:  1.0108233901854247\tMSE_testing:  1.0819492819728933\n",
      "Epoca numero 3\tMSE_training:  1.0113471394271962\tMSE_testing:  1.0820587785562723\n",
      "Epoca numero 4\tMSE_training:  1.0124075488172641\tMSE_testing:  1.0829533933458317\n",
      "Epoca numero 5\tMSE_training:  1.0148268762735486\tMSE_testing:  1.086254080079729\n",
      "Epoca numero 6\tMSE_training:  1.0224672281587488\tMSE_testing:  1.1000452715363342\n",
      "Epoca numero 7\tMSE_training:  1.0567017726775427\tMSE_testing:  1.171106396064057\n",
      "Epoca numero 8\tMSE_training:  1.1898973541713456\tMSE_testing:  1.3866040677475384\n",
      "Epoca numero 9\tMSE_training:  1.404629319379203\tMSE_testing:  1.5979051209395359\n",
      "Epoca numero 10\tMSE_training:  1.5558272121433137\tMSE_testing:  1.713227319909101\n",
      "Epoca numero 11\tMSE_training:  1.6349323056811438\tMSE_testing:  1.7734809262425963\n",
      "Epoca numero 12\tMSE_training:  1.6780364863923118\tMSE_testing:  1.8089747766584259\n",
      "Epoca numero 13\tMSE_training:  1.7047056684292163\tMSE_testing:  1.832488193192243\n",
      "Epoca numero 14\tMSE_training:  1.7231016432435153\tMSE_testing:  1.849277874355959\n",
      "Epoca numero 15\tMSE_training:  1.736716285089677\tMSE_testing:  1.8617947073841552\n",
      "Epoca numero 16\tMSE_training:  1.7472488062804963\tMSE_testing:  1.8713955062771515\n",
      "Epoca numero 17\tMSE_training:  1.7556613318647434\tMSE_testing:  1.878938863706901\n",
      "Epoca numero 18\tMSE_training:  1.7625625943895307\tMSE_testing:  1.885006169287137\n",
      "Epoca numero 19\tMSE_training:  1.7683600109760411\tMSE_testing:  1.8900003358023556\n",
      "Epoca numero 20\tMSE_training:  1.7733320781349888\tMSE_testing:  1.894202221174749\n",
      "Epoca numero 21\tMSE_training:  1.7776707668871954\tMSE_testing:  1.897808526074961\n",
      "Epoca numero 22\tMSE_training:  1.7815097017164956\tMSE_testing:  1.9009583962489098\n",
      "Epoca numero 23\tMSE_training:  1.7849435304126058\tMSE_testing:  1.903751808068147\n",
      "Epoca numero 24\tMSE_training:  1.7880410328134833\tMSE_testing:  1.9062619074177591\n",
      "Epoca numero 25\tMSE_training:  1.7908537302869127\tMSE_testing:  1.9085431196961\n",
      "Epoca numero 26\tMSE_training:  1.7934213964477628\tMSE_testing:  1.9106364729642433\n",
      "Epoca numero 27\tMSE_training:  1.7957755456275923\tMSE_testing:  1.912573166552092\n",
      "Epoca numero 28\tMSE_training:  1.797941660853936\tMSE_testing:  1.9143770539537697\n",
      "Epoca numero 29\tMSE_training:  1.7999406564789064\tMSE_testing:  1.9160664388530264\n"
     ]
    }
   ],
   "source": [
    "'''REGRESSION'''\n",
    "\n",
    "output_string = 'Weight'\n",
    "input_totale = dataset_training.loc[:len(dataset_training), dataset_training.columns != output_string]\n",
    "output_totale = dataset_training.loc[:len(dataset_training), dataset_training.columns == output_string]\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, learning_rate, len_input, num_layers, num_layer_nodes, epochs=1):\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.len_input = len_input\n",
    "        self.num_layers = num_layers\n",
    "        self.num_layer_nodes = num_layer_nodes\n",
    "        self.weight_list = []\n",
    "        self.layer_list = []\n",
    "        self.derivate_list = []\n",
    "        self.threshold = 0.5\n",
    "        self.totalerror = 0\n",
    "        self.prediction = None #serve per il testing dell'errore\n",
    "\n",
    "\n",
    "        #inizializzazione matrice dei pesi\n",
    "        for i in range(self.num_layers-1):\n",
    "            #new_matrix = np.full((self.num_layer_nodes[i], self.num_layer_nodes[i+1]), 0.99) #matrice con righe=i e colonne i+1: (input,output) di un layer\n",
    "            new_matrix = np.random.rand(self.num_layer_nodes[i], self.num_layer_nodes[i+1]) #matrice con righe=i e colonne i+1: (input,output) di un layer\n",
    "            self.weight_list.append(new_matrix)\n",
    "\n",
    "        #inizializzazione layer\n",
    "        for i in range(self.num_layers):\n",
    "            self.layer_list.append(np.zeros(self.num_layer_nodes[i]))\n",
    "\n",
    "        #inizializzazione lista derivate parziali\n",
    "        for i in range(self.num_layers-1): #non viene contato il layer di input\n",
    "            self.derivate_list.append(np.zeros(self.num_layer_nodes[i+1]))\n",
    "\n",
    "\n",
    "        #print(\"weight: \\n\", self.weight_list)\n",
    "        #print(\"derivate: \\n\", self.derivate_list)\n",
    "        #print(\"layer: \\n\". self.layer_list)\n",
    "\n",
    "\n",
    "    def activation(self, x, type='sigmoid'): \n",
    "        if type=='sigmoid':\n",
    "            return 1/(1+np.exp(-x))\n",
    "        elif type=='tanh':\n",
    "            return (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))\n",
    "        elif type=='relu':\n",
    "            return (x>0)*x\n",
    "        return None\n",
    "\n",
    "    def activation_derivate(self, x, type='sigmoid'):\n",
    "        if type=='sigmoid':\n",
    "            return (1-self.activation(x, type='sigmoid'))*self.activation(x, type='sigmoid')\n",
    "        elif type=='tanh':\n",
    "            return 1-np.tanh(x)**2\n",
    "        elif type=='relu':\n",
    "            return (x>0)*1\n",
    "        return None\n",
    "    \n",
    "    def training(self, dataset_training, output_training, output_testing, input_testing):\n",
    "        '''inizializzazione array output per il calcolo dell'errore'''\n",
    "        self.prediction = np.zeros(dataset_training.shape[0])\n",
    "\n",
    "        for e in range(self.epochs):\n",
    "            #self.learning_rate = 1/(1000+self.epochs*20)\n",
    "            print(\"Epoca numero\", e, end='\\t')\n",
    "            for i in range(dataset_training.shape[0]): #ciclo che itera per ogni sample del dataset \n",
    "                sample = dataset_training.loc[i:i]\n",
    "                self.feedforwarding(sample)\n",
    "                '''salvataggio output per test errore'''\n",
    "                self.prediction[i] = self.layer_list[-1][0][0]\n",
    "                self.backpropagation(i)\n",
    "\n",
    "            print(\"MSE_training: \", self.dataset_MSE(self.prediction, output_training), end='\\t')\n",
    "            '''test error'''\n",
    "            print(\"MSE_testing: \", self.dataset_MSE(self.testing(input_testing), output_testing))\n",
    "\n",
    "\n",
    "\n",
    "    def testing(self, dataset_testing) -> list: \n",
    "        predicted_out = []\n",
    "        for i in range(dataset_testing.shape[0]):\n",
    "            self.feedforwarding(dataset_testing.loc[i:i])\n",
    "            predicted_out.append(self.layer_list[-1][0][0])\n",
    "\n",
    "        return predicted_out\n",
    "\n",
    "    def feedforwarding(self, sample):\n",
    "        '''fase di feedforwarding'''\n",
    "        self.layer_list[0] = sample.values #inizializzo l'input della rete ai valori del sample\n",
    "        for layer_index in range(self.num_layers-1): # per ogni weight_matrix esistente\n",
    "            out_unactivated = np.dot(self.layer_list[layer_index], self.weight_list[layer_index])\n",
    "            if layer_index < self.num_layers-2:\n",
    "                self.layer_list[layer_index+1] = self.activation(out_unactivated)\n",
    "                #print(\"Assigned layer list1: \\n\\n\", self.layer_list[layer_index+1])\n",
    "            else:\n",
    "                self.layer_list[layer_index+1] = out_unactivated\n",
    "                #print(\"Assigned layer list2: \\n\\n\", self.layer_list[layer_index+1])\n",
    "\n",
    "    def backpropagation(self, i): \n",
    "        '''fase di backtracking'''\n",
    "\n",
    "        error = -(output_totale.loc[i:i]-self.layer_list[-1]).values \n",
    "        self.derivate_list[-1] = error*self.activation_derivate(self.layer_list[-1])\n",
    "        \n",
    "        for j in range(num_layers-2, 0, -1): #3-2 layer: si parte dall'indice 1 con 0 escluso (es.)\n",
    "            ad = self.activation_derivate(self.layer_list[j])\n",
    "            \n",
    "            summatory = np.dot(self.derivate_list[j], self.weight_list[j].T )\n",
    "            self.derivate_list[j-1] = ad*summatory\n",
    "            '''aggiornamento dei pesi'''\n",
    "            self.weight_list[j] -= self.learning_rate*np.matmul(self.layer_list[j].T, self.derivate_list[j])\n",
    "    \n",
    "        self.weight_list[0] -= self.learning_rate*np.matmul(self.layer_list[0].T, self.derivate_list[0])\n",
    "    \n",
    "    def MSE(self, input_, prediction):\n",
    "        return (input_-prediction)**2\n",
    "    \n",
    "    def dataset_MSE(self, prediction, output):\n",
    "        return np.mean(self.MSE(prediction, output))\n",
    "\n",
    "epochs = 30\n",
    "learning_rate = 0.01\n",
    "len_input = len(input_totale.columns)\n",
    "num_layer_nodes = [len_input, 10, 10, 1] #numero nodi di ciascun layer da sinistra verso destra\n",
    "num_layers = len(num_layer_nodes) #numero di layer compresi nella rete neurale input e output compresi\n",
    "\n",
    "\n",
    "output_training = np.array(dataset_training.loc[:, dataset_test.columns == output_string])\n",
    "n = NeuralNetwork(learning_rate=learning_rate, len_input=len_input, \n",
    "                num_layers=num_layers, num_layer_nodes=num_layer_nodes, epochs=epochs,\n",
    "                )\n",
    "\n",
    "output_testing = dataset_test.loc[:, dataset_test.columns == output_string].values\n",
    "dataset_test = dataset_test.reset_index(drop=True)\n",
    "X_test = dataset_test.loc[:, dataset_test.columns != output_string]\n",
    "n.training(input_totale, output_training, output_testing, X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 -655.4659826124196\n",
      "47397.6232361115\n"
     ]
    }
   ],
   "source": [
    "'''fase di testing'''\n",
    "\n",
    "dataset_test = dataset_test.reset_index(drop=True)\n",
    "X_test = dataset_test.loc[:, dataset_test.columns != output_string]\n",
    "\n",
    "\n",
    "predicted_out = np.array(n.testing(X_test))\n",
    "Y_effettivo = np.array(dataset_test[output_string])\n",
    "\n",
    "print(max(predicted_out), min(predicted_out))\n",
    "total_error = np.mean(n.MSE(predicted_out, Y_effettivo))\n",
    "print(total_error)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
