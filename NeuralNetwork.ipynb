{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random as rn\n",
    "import pandas as pd\n",
    "from library import Map, GradientDescent\n",
    "import time\n",
    "from random import randrange\n",
    "\n",
    "#profiling libraries\n",
    "import cProfile\n",
    "import pstats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''REGRESSION'''\n",
    "\n",
    "'''Downloading dataset'''\n",
    "dataset = pd.read_csv(\"/Users/alessandrococcia/Downloads/ObesityDataSet.csv\")\n",
    "\n",
    "'''Dataset sampling'''\n",
    "dataset = dataset.sample(frac=1, ignore_index=True) #shuffle sample in the training set\n",
    "\n",
    "'''mapping delle stringhe'''\n",
    "dataset = pd.get_dummies(dataset, drop_first=True, dtype=float)\n",
    "\n",
    "'''normalization'''\n",
    "dataset = (dataset-dataset.mean())/dataset.std()\n",
    "\n",
    "'''Inserimento colonna di bias'''\n",
    "dataset.insert(0, \"Bias\", np.ones(len(dataset)), True) #Bias row\n",
    "\n",
    "\n",
    "TRAIN_TEST_SPLIT_PERCENTAGE = 0.9\n",
    "dataset_training = dataset[:int(len(dataset) * TRAIN_TEST_SPLIT_PERCENTAGE)]\n",
    "dataset_test = dataset[int(len(dataset) * TRAIN_TEST_SPLIT_PERCENTAGE):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yq/gyf11wn125qgkzlcv24mbp3w0000gn/T/ipykernel_67114/3248209574.py:11: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  dataset.replace(to_replace=(\"Obesity_Type_I\", \"Obesity_Type_II\", \"Obesity_Type_III\"), value=1, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "'''CLASSIFICATION'''\n",
    "\n",
    "'''Downloading dataset'''\n",
    "dataset = pd.read_csv(\"/Users/alessandrococcia/Downloads/ObesityDataSet.csv\")\n",
    "\n",
    "'''Dataset sampling'''\n",
    "dataset = dataset.sample(frac=1, ignore_index=True) #shuffle sample in the training set\n",
    "\n",
    "'''mapping delle stringhe'''\n",
    "dataset.replace(to_replace=(\"Insufficient_Weight\", \"Normal_Weight\", \"Overweight_Level_I\", \"Overweight_Level_II\"), value=0, inplace=True)\n",
    "dataset.replace(to_replace=(\"Obesity_Type_I\", \"Obesity_Type_II\", \"Obesity_Type_III\"), value=1, inplace=True)\n",
    "dataset = pd.get_dummies(dataset, drop_first=True, dtype=float)\n",
    "\n",
    "'''normalization'''\n",
    "dataset = (dataset-dataset.mean())/dataset.std()\n",
    "\n",
    "'''Inserimento colonna di bias'''\n",
    "dataset.insert(0, \"Bias\", np.ones(len(dataset)), True) #Bias row\n",
    "\n",
    "\n",
    "TRAIN_TEST_SPLIT_PERCENTAGE = 0.9\n",
    "dataset_training = dataset[:int(len(dataset) * TRAIN_TEST_SPLIT_PERCENTAGE)]\n",
    "dataset_test = dataset[int(len(dataset) * TRAIN_TEST_SPLIT_PERCENTAGE):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoca numero 0\n",
      "Mean Squared Error: 0.05119007048411901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but MLPRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "'''CLASSIFICATION'''\n",
    "\n",
    "output_string = 'NObeyesdad'\n",
    "input_totale = dataset_training.loc[:len(dataset_training), dataset_training.columns != output_string]\n",
    "output_totale = dataset_training.loc[:len(dataset_training), dataset_training.columns == output_string]\n",
    "\n",
    "'''Values to determine threshold'''\n",
    "positive_value, negative_value = max(output_totale['NObeyesdad'].unique()), min(output_totale['NObeyesdad'].unique())\n",
    "threshold = 0.5\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, learning_rate, len_input, num_layers, num_layer_nodes, epochs=1):\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.len_input = len_input\n",
    "        self.num_layers = num_layers\n",
    "        self.num_layer_nodes = num_layer_nodes\n",
    "        self.weight_list = []\n",
    "        self.layer_list = []\n",
    "        self.derivate_list = []\n",
    "        self.threshold = 0.5\n",
    "        self.totalerror = 0\n",
    "\n",
    "\n",
    "        #inizializzazione matrice dei pesi\n",
    "        for i in range(self.num_layers-1):\n",
    "            new_matrix = np.random.rand(self.num_layer_nodes[i], self.num_layer_nodes[i+1]) #matrice con righe=i e colonne i+1: (input,output) di un layer\n",
    "            self.weight_list.append(new_matrix)\n",
    "\n",
    "        #inizializzazione layer\n",
    "        for i in range(self.num_layers):\n",
    "            self.layer_list.append(np.zeros(self.num_layer_nodes[i]))\n",
    "\n",
    "        #inizializzazione lista derivate parziali\n",
    "        for i in range(self.num_layers-1): #non viene contato il layer di input\n",
    "            self.derivate_list.append(np.zeros(self.num_layer_nodes[i+1]))\n",
    "\n",
    "    def activation(self, x, type='sigmoid'): \n",
    "        if type=='sigmoid':\n",
    "            return 1/(1+np.exp(-x))\n",
    "        elif type=='tanh':\n",
    "            return (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))\n",
    "        return None\n",
    "\n",
    "    def activation_derivative(self, x, type='sigmoid'):\n",
    "        if type=='sigmoid':\n",
    "            return (1-self.activation(x, type='sigmoid'))*self.activation(x, type='sigmoid')\n",
    "        return None\n",
    "    \n",
    "    def training(self, dataset_training):\n",
    "        for e in range(self.epochs):\n",
    "            print(\"Epoca numero\", e)\n",
    "            for i in range(dataset_training.shape[0]): #ciclo che itera per ogni sample del dataset \n",
    "                sample = dataset_training.loc[i:i]\n",
    "                self.feedforwarding(sample)\n",
    "                #print(\"Error: \", self.totalerror)\n",
    "                self.backpropagation(i)\n",
    "\n",
    "\n",
    "    def testing(self, dataset_testing) -> list: \n",
    "        predicted_out = []\n",
    "        for i in range(dataset_testing.shape[0]):\n",
    "            self.feedforwarding(dataset_testing.loc[i:i])\n",
    "            predicted_out.append(n.layer_list[-1][0][0])\n",
    "\n",
    "        return predicted_out\n",
    "\n",
    "    def feedforwarding(self, sample):\n",
    "        '''fase di feedforwarding'''\n",
    "        self.layer_list[0] = np.array(sample) #inizializzo l'input della rete ai valori del sample\n",
    "        for layer_index in range(self.num_layers-1): # per ogni weight_matrix esistente\n",
    "            out_unactivated = np.dot(self.layer_list[layer_index], self.weight_list[layer_index])#+0.25 #+ bias test\n",
    "            #print(out_unactivated)\n",
    "            self.layer_list[layer_index+1] = self.activation(out_unactivated)\n",
    "            #print(self.layer_list)\n",
    "\n",
    "    def backpropagation(self, i): \n",
    "        '''fase di backtracking'''\n",
    "        \n",
    "        #inizializzazione errore output\n",
    "        error = -(self.activation(output_totale.loc[i:i])-self.layer_list[-1])\n",
    "        self.derivate_list[-1] = error\n",
    "        for j in range(num_layers-2, 0, -1): #3-2 layer: si parte dall'indice 1 con 0 escluso (es.)\n",
    "            \n",
    "            ad = self.activation_derivative(self.layer_list[j])\n",
    "            summatory = np.matmul(self.weight_list[j], self.derivate_list[j])\n",
    "            self.derivate_list[j-1] = np.multiply(ad, summatory.T).T\n",
    "            '''aggiornamento dei pesi'''\n",
    "            self.weight_list[j] -= self.learning_rate* np.outer(self.layer_list[j], self.derivate_list[j])\n",
    "\n",
    "        #manca l'aggiornamento del peso all'indice 0 che non Ã¨ calcolato nell'ultimo ciclo\n",
    "        self.weight_list[0] -= learning_rate*np.outer(self.layer_list[0], self.derivate_list[0])\n",
    "\n",
    "    def binary_cross_entropy(y_true, y_pred):\n",
    "        epsilon = 1e-15  # Piccola costante per evitare divisioni per zero\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)  # Clip dei valori per evitare valori di logaritmo negativi\n",
    "        return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "        # Calcolo dell'errore per un singolo esempio di training o testing\n",
    "        error = binary_cross_entropy(y_true, y_pred)\n",
    "\n",
    "epochs = 1\n",
    "learning_rate = 1\n",
    "len_input = len(input_totale.columns)\n",
    "num_layer_nodes = [len_input, 100, 1] #numero nodi di ciascun layer da sinistra verso destra\n",
    "num_layers = len(num_layer_nodes) #numero di layer compresi nella rete neurale input e output compresi\n",
    "\n",
    "n = NeuralNetwork(learning_rate=learning_rate, len_input=len_input, \n",
    "                num_layers=num_layers, num_layer_nodes=num_layer_nodes, epochs=epochs\n",
    "                )\n",
    "n.training(input_totale)\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "'''fase di testing'''\n",
    "\n",
    "dataset_test = dataset_test.reset_index(drop=True)\n",
    "X_test = dataset_test.loc[:, dataset_test.columns != output_string]\n",
    "\n",
    "\n",
    "'''fase di testing'''\n",
    "predicted_out = n.testing(X_test)\n",
    "Y_effettivo = np.array(dataset_test[output_string])\n",
    "real_out = np.array(1/(1+np.exp(-Y_effettivo)))\n",
    "\n",
    "\n",
    "# Crea un'istanza di LabelEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Converti le etichette di classe continue in etichette di classe discrete\n",
    "output_totale_discrete = label_encoder.fit_transform(output_totale.values.ravel())\n",
    "\n",
    "# Definisci e addestra il modello di rete neurale\n",
    "model = MLPRegressor(hidden_layer_sizes=(100,100,), activation='logistic', solver='sgd', random_state=42)\n",
    "model.fit(np.array(input_totale), np.array(output_totale_discrete))\n",
    "\n",
    "# Esegui le previsioni sul test set\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "mse = np.mean((real_out-predictions)**2)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoca numero 0\tMSE:  1130.4675811285288\n",
      "Epoca numero 1\tMSE:  1073.6381259274124\n",
      "Epoca numero 2\tMSE:  987.4557759305452\n",
      "Epoca numero 3\tMSE:  819.3243175768932\n",
      "Epoca numero 4\tMSE:  537.1421371384573\n",
      "Epoca numero 5\tMSE:  359.07822874477625\n",
      "Epoca numero 6\tMSE:  248.12149973049557\n",
      "Epoca numero 7\tMSE:  151.61973915509282\n",
      "Epoca numero 8\tMSE:  73.92081128423021\n",
      "Epoca numero 9\tMSE:  13.678513928283516\n",
      "Epoca numero 10\tMSE:  8.770381118009023\n",
      "Epoca numero 11\tMSE:  5.600697959164512\n",
      "Epoca numero 12\tMSE:  3.5457896211278563\n",
      "Epoca numero 13\tMSE:  2.4199401855991978\n",
      "Epoca numero 14\tMSE:  1.946565499678596\n",
      "Epoca numero 15\tMSE:  1.7907173230382913\n",
      "Epoca numero 16\tMSE:  1.7536788131872372\n",
      "Epoca numero 17\tMSE:  1.7572240365234306\n",
      "Epoca numero 18\tMSE:  1.7738664392094594\n",
      "Epoca numero 19\tMSE:  1.7938072105206506\n",
      "Epoca numero 20\tMSE:  1.8134820748021185\n",
      "Epoca numero 21\tMSE:  1.8315896529734066\n",
      "Epoca numero 22\tMSE:  1.8476931566996806\n",
      "Epoca numero 23\tMSE:  1.8617235946332968\n",
      "Epoca numero 24\tMSE:  1.873785017442138\n",
      "Epoca numero 25\tMSE:  1.8840626708807462\n",
      "Epoca numero 26\tMSE:  1.8927724464687559\n",
      "Epoca numero 27\tMSE:  1.9001317498973656\n",
      "Epoca numero 28\tMSE:  1.9063435619841265\n",
      "Epoca numero 29\tMSE:  1.9115889347755937\n",
      "Epoca numero 30\tMSE:  1.916024673247126\n",
      "Epoca numero 31\tMSE:  1.9197839664726342\n",
      "Epoca numero 32\tMSE:  1.922978505416791\n",
      "Epoca numero 33\tMSE:  1.9257011907749253\n",
      "Epoca numero 34\tMSE:  1.9280289205547256\n",
      "Epoca numero 35\tMSE:  1.9300251936743196\n",
      "Epoca numero 36\tMSE:  1.931742414057464\n",
      "Epoca numero 37\tMSE:  1.9332238634640562\n",
      "Epoca numero 38\tMSE:  1.9345053551263043\n",
      "Epoca numero 39\tMSE:  1.9356166005246322\n",
      "Epoca numero 40\tMSE:  1.9365823286580215\n",
      "Epoca numero 41\tMSE:  1.9374231971878126\n",
      "Epoca numero 42\tMSE:  1.938156531504731\n",
      "Epoca numero 43\tMSE:  1.9387969231270914\n",
      "Epoca numero 44\tMSE:  1.9393567139725776\n",
      "Epoca numero 45\tMSE:  1.9398463884922479\n",
      "Epoca numero 46\tMSE:  1.9402748916409138\n",
      "Epoca numero 47\tMSE:  1.9406498872426305\n",
      "Epoca numero 48\tMSE:  1.9409779684708175\n",
      "Epoca numero 49\tMSE:  1.9412648298380433\n"
     ]
    }
   ],
   "source": [
    "'''REGRESSION'''\n",
    "\n",
    "output_string = 'Weight'\n",
    "input_totale = dataset_training.loc[:len(dataset_training), dataset_training.columns != output_string]\n",
    "output_totale = dataset_training.loc[:len(dataset_training), dataset_training.columns == output_string]\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, learning_rate, len_input, num_layers, num_layer_nodes, epochs=1):\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.len_input = len_input\n",
    "        self.num_layers = num_layers\n",
    "        self.num_layer_nodes = num_layer_nodes\n",
    "        self.weight_list = []\n",
    "        self.layer_list = []\n",
    "        self.derivate_list = []\n",
    "        self.threshold = 0.5\n",
    "        self.totalerror = 0\n",
    "        self.prediction = None #serve per il testing dell'errore\n",
    "\n",
    "\n",
    "        #inizializzazione matrice dei pesi\n",
    "        for i in range(self.num_layers-1):\n",
    "            new_matrix = np.random.rand(self.num_layer_nodes[i], self.num_layer_nodes[i+1]) #matrice con righe=i e colonne i+1: (input,output) di un layer\n",
    "            self.weight_list.append(new_matrix)\n",
    "\n",
    "        #inizializzazione layer\n",
    "        for i in range(self.num_layers):\n",
    "            self.layer_list.append(np.zeros(self.num_layer_nodes[i]))\n",
    "\n",
    "        #inizializzazione lista derivate parziali\n",
    "        for i in range(self.num_layers-1): #non viene contato il layer di input\n",
    "            self.derivate_list.append(np.zeros(self.num_layer_nodes[i+1]))\n",
    "\n",
    "\n",
    "        #print(\"weight: \\n\", self.weight_list)\n",
    "        #print(\"derivate: \\n\", self.derivate_list)\n",
    "        #print(\"layer: \\n\". self.layer_list)\n",
    "\n",
    "\n",
    "    def activation(self, x, type='sigmoid'): \n",
    "        if type=='sigmoid':\n",
    "            return 1/(1+np.exp(-x))\n",
    "        elif type=='tanh':\n",
    "            return (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))\n",
    "        elif type=='relu':\n",
    "            return (x>0)*x\n",
    "        return None\n",
    "\n",
    "    def activation_derivate(self, x, type='sigmoid'):\n",
    "        if type=='sigmoid':\n",
    "            return (1-self.activation(x, type='sigmoid'))*self.activation(x, type='sigmoid')\n",
    "        elif type=='relu':\n",
    "            return (x>0)*1\n",
    "        return None\n",
    "    \n",
    "    def training(self, dataset_training, output):\n",
    "        '''inizializzazione array output per il calcolo dell'errore'''\n",
    "        self.prediction = np.zeros(dataset_training.shape[0])\n",
    "\n",
    "        for e in range(self.epochs):\n",
    "            #self.learning_rate = 1/(1000+self.epochs*20)\n",
    "            print(\"Epoca numero\", e, end='\\t')\n",
    "            for i in range(dataset_training.shape[0]): #ciclo che itera per ogni sample del dataset \n",
    "                sample = dataset_training.loc[i:i]\n",
    "                self.feedforwarding(sample)\n",
    "                '''salvataggio output per test errore'''\n",
    "                self.prediction[i] = self.layer_list[-1][0][0]\n",
    "                self.backpropagation(i)\n",
    "\n",
    "            print(\"MSE: \", self.dataset_MSE(self.prediction, output))\n",
    "\n",
    "\n",
    "    def testing(self, dataset_testing) -> list: \n",
    "        predicted_out = []\n",
    "        for i in range(dataset_testing.shape[0]):\n",
    "            self.feedforwarding(dataset_testing.loc[i:i])\n",
    "            predicted_out.append(self.layer_list[-1][0][0])\n",
    "\n",
    "        return predicted_out\n",
    "\n",
    "    def feedforwarding(self, sample):\n",
    "        '''fase di feedforwarding'''\n",
    "        self.layer_list[0] = sample.values #inizializzo l'input della rete ai valori del sample\n",
    "        for layer_index in range(self.num_layers-1): # per ogni weight_matrix esistente\n",
    "            out_unactivated = np.dot(self.layer_list[layer_index], self.weight_list[layer_index])\n",
    "            if layer_index < self.num_layers-2:\n",
    "                self.layer_list[layer_index+1] = self.activation(out_unactivated)\n",
    "                #print(\"Assigned layer list1: \\n\\n\", self.layer_list[layer_index+1])\n",
    "            else:\n",
    "                self.layer_list[layer_index+1] = out_unactivated\n",
    "                #print(\"Assigned layer list2: \\n\\n\", self.layer_list[layer_index+1])\n",
    "\n",
    "    def backpropagation(self, i): \n",
    "        '''fase di backtracking'''\n",
    "\n",
    "        error = -(output_totale.loc[i:i]-self.layer_list[-1]).values \n",
    "        self.derivate_list[-1] = error*self.activation_derivate(self.layer_list[-1])\n",
    "        \n",
    "        for j in range(num_layers-2, 0, -1): #3-2 layer: si parte dall'indice 1 con 0 escluso (es.)\n",
    "            ad = self.activation_derivate(self.layer_list[j])\n",
    "            \n",
    "            summatory = self.derivate_list[j]@self.weight_list[j].T #matmul o dot?\n",
    "            self.derivate_list[j-1] = ad*summatory\n",
    "            '''aggiornamento dei pesi'''\n",
    "            self.weight_list[j] -= self.learning_rate*np.matmul(self.layer_list[j].T, self.derivate_list[j])\n",
    "    \n",
    "        self.weight_list[0] -= self.learning_rate*np.matmul(self.layer_list[0].T, self.derivate_list[0])\n",
    "    \n",
    "    def MSE(self, input, prediction):\n",
    "        return (input-prediction)**2\n",
    "    \n",
    "    def dataset_MSE(self, prediction, output):\n",
    "        return np.mean(self.MSE(prediction, output))\n",
    "\n",
    "epochs = 50\n",
    "learning_rate = 0.01\n",
    "len_input = len(input_totale.columns)\n",
    "num_layer_nodes = [len_input, 100, 1] #numero nodi di ciascun layer da sinistra verso destra\n",
    "num_layers = len(num_layer_nodes) #numero di layer compresi nella rete neurale input e output compresi\n",
    "\n",
    "\n",
    "output = np.array(dataset_training.loc[:, dataset_test.columns == output_string])\n",
    "n = NeuralNetwork(learning_rate=learning_rate, len_input=len_input, \n",
    "                num_layers=num_layers, num_layer_nodes=num_layer_nodes, epochs=epochs,\n",
    "                )\n",
    "\n",
    "n.training(input_totale, output)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8401444995343148 -1.7599733674122124\n",
      "0.06757057412882364\n"
     ]
    }
   ],
   "source": [
    "'''fase di testing'''\n",
    "\n",
    "dataset_test = dataset_test.reset_index(drop=True)\n",
    "X_test = dataset_test.loc[:, dataset_test.columns != output_string]\n",
    "\n",
    "\n",
    "'''fase di testing'''\n",
    "predicted_out = np.array(n.testing(X_test))\n",
    "Y_effettivo = np.array(dataset_test[output_string])\n",
    "#real_out = np.array(1/(1+np.exp(-Y_effettivo)))\n",
    "\n",
    "\n",
    "print(max(predicted_out), min(predicted_out))\n",
    "total_error = np.mean(n.MSE(predicted_out, Y_effettivo))\n",
    "print(total_error)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
