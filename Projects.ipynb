{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d94ea7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random as rn\n",
    "import pandas as pd\n",
    "from library import Map, GradientDescent\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from random import randrange\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "51a9d4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Downloading dataset'''\n",
    "dataset = pd.read_csv(\"/Users/alessandrococcia/Downloads/ObesityDataSet.csv\")\n",
    "dataset = dataset.sample(frac=1) #shuffle sample in the training set\n",
    "\n",
    "'''mapping delle stringhe'''\n",
    "\n",
    "#dataset = pd.get_dummies(dataset, drop_first=True).astype(float)\n",
    "\n",
    "'''normalization'''\n",
    "dataset = (dataset-dataset.min())/(dataset.max()-dataset.min())\n",
    "\n",
    "'''Inserimento colonna di bias'''\n",
    "dataset.insert(0, \"Bias\", np.ones(len(dataset)), True) #Bias row\n",
    "\n",
    "\n",
    "\n",
    "TRAIN_TEST_SPLIT_PERCENTAGE = 0.9\n",
    "dataset_training = dataset[:int(len(dataset) * TRAIN_TEST_SPLIT_PERCENTAGE)]\n",
    "dataset_test = dataset[int(len(dataset) * TRAIN_TEST_SPLIT_PERCENTAGE):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b32a522f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Map:\n",
    "    def __init__(self):\n",
    "        self.count = 0\n",
    "        self.count_dataset = {}\n",
    "        self.d = {}\n",
    "        self.dataset_dictionary = {}\n",
    "        self.matrice = None\n",
    "        self.dataset = None\n",
    "        len = None\n",
    "    \n",
    "    def mappingElement(self, string):\n",
    "        if type(string) is str:\n",
    "            if self.d.get(string) is not None:\n",
    "                return self.d.get(string)\n",
    "            else:\n",
    "                self.count = self.count+1\n",
    "                self.d[string] = self.count\n",
    "                return self.count\n",
    "        return float(string)\n",
    "    \n",
    "    def mappingMatrix(self, matrice):\n",
    "        self.matrice = None\n",
    "        self.matrice = np.array(matrice)\n",
    "        len = self.matrice.shape\n",
    "        for g in range(len[0]):\n",
    "            for j in range(len[1]):\n",
    "                self.matrice[g][j] = self.mappingElement(self.matrice[g][j])\n",
    "        return np.float64(self.matrice.copy())\n",
    "    \n",
    "    def mappingElementDataset(self, value, colonna):\n",
    "        if type(value) is str:\n",
    "            if self.dataset_dictionary[colonna].get(value) is not None:\n",
    "                return self.dataset_dictionary[colonna].get(value)\n",
    "            else:\n",
    "                if self.count_dataset.get(colonna) is None:\n",
    "                    self.count_dataset[colonna] = 0\n",
    "                    self.dataset_dictionary[colonna][value] = self.count_dataset[colonna]\n",
    "                    return self.count_dataset[colonna]\n",
    "                self.count_dataset[colonna] += 1\n",
    "                self.dataset_dictionary[colonna][value] = self.count_dataset[colonna]\n",
    "                return self.count_dataset[colonna]\n",
    "        return value\n",
    "    \n",
    "    def mappingDataset(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        colonne = dataset.columns\n",
    "        length = len(dataset)\n",
    "        for column in colonne:\n",
    "            self.dataset_dictionary[column] = {}\n",
    "            for index in range(length):\n",
    "                dataset.at[index, column] = self.mappingElementDataset(dataset.at[index, column], column)\n",
    "        return self.dataset\n",
    "\n",
    "    def MSE(self, a, b):\n",
    "        return np.mean((np.square(a - b)))\n",
    "    def RMSE(self, a, b):\n",
    "        return math.sqrt(np.mean(np.square(a - b)))\n",
    "    def MAE(self, a, b):\n",
    "        return np.mean(abs(a-b))\n",
    "    \n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, outputstring, positive, negative): \n",
    "        self.outputstring = outputstring\n",
    "        self.positive = positive\n",
    "        self.negative = negative\n",
    "        self.nodes = []\n",
    "        self.label = None\n",
    "        self.root = False\n",
    "\n",
    "    def LearnDecisionTree(self, examples, attributes, parent_examples, column_values):\n",
    "        self.root = True\n",
    "        self.LearnDecisionTreeAux_(examples, attributes, parent_examples, column_values)\n",
    "\n",
    "    def LearnDecisionTreeAux_(self, examples, attributes, parent_examples, column_values):\n",
    "        same_classification = self.SameClassification(examples)    \n",
    "        \n",
    "        if len(examples.loc[:, examples.columns != self.outputstring]) == 0:\n",
    "            return self.PluralityValue(parent_examples)\n",
    "        elif same_classification is not False:\n",
    "            return same_classification\n",
    "        elif len(attributes) == 0: \n",
    "            return self.PluralityValue(examples)\n",
    "        else:\n",
    "            bestattribute = self.Importance(attributes, examples)\n",
    "            self.label = bestattribute #Seleziono l'attributo migliore\n",
    "            \n",
    "            for value in self.Values1(bestattribute, column_values):\n",
    "                remainingexamples = self.Examples(bestattribute, examples, value)\n",
    "                tree = DecisionTree(self.outputstring, self.positive, self.negative)\n",
    "                attributes_left = self.PopListValue(attributes.copy(), bestattribute)                \n",
    "                subtree = tree.LearnDecisionTreeAux_(remainingexamples.loc[:, remainingexamples.columns != bestattribute], attributes_left, examples, column_values)\n",
    "                self.nodes.append((value, subtree))\n",
    "        return self\n",
    "        \n",
    "    def Importance(self, attributes, examples): \n",
    "        max = -1\n",
    "        ret = None\n",
    "        for a in attributes:\n",
    "            loc = self.Gain(examples, a)\n",
    "            if loc > max:\n",
    "                max = loc\n",
    "                ret = a\n",
    "        return ret\n",
    "        \n",
    "    def B(self, q):\n",
    "        if q == 1 or q == 0:\n",
    "            return 0\n",
    "        return -(q*math.log2(q)+(1-q)*math.log2(1-q))\n",
    "    \n",
    "    def Remainder(self, examples, attribute, p, n):\n",
    "        sum = 0\n",
    "        \n",
    "        for v in self.Values2(attribute, examples):\n",
    "            if type(v) is not str and math.isnan(float(v)):\n",
    "                pk = len(examples.loc[(examples[self.outputstring] == self.positive) & (examples[attribute].isnull())])\n",
    "                nk = len(examples.loc[(examples[self.outputstring] == self.negative) & (examples[attribute].isnull())])\n",
    "            else:\n",
    "                pk = len(examples.loc[(examples[self.outputstring] == self.positive) & (examples[attribute] == v)])\n",
    "                nk = len(examples.loc[(examples[self.outputstring] == self.negative) & ((examples[attribute] == v))])\n",
    "            \n",
    "            partial = (pk+nk)*self.B(pk/(pk+nk))\n",
    "            sum += partial\n",
    "        return (1/(p+n))*sum\n",
    "    \n",
    "    def Gain(self, examples, attribute):\n",
    "        p = len(examples.loc[examples[self.outputstring] == self.positive])\n",
    "        n = len(examples.loc[examples[self.outputstring] == self.negative])\n",
    "        b = self.B(p/(p+n))\n",
    "        r = self.Remainder(examples, attribute, p, n)\n",
    "        return (b-r)\n",
    "        \n",
    "    def PluralityValue(self, parent_examples): \n",
    "        '''Selects the most common ouput value among a set of examples, breaking ties randomly'''\n",
    "        value, max = [], 0\n",
    "        d = self.CreateDictionary(parent_examples)\n",
    "        for key in d.keys():\n",
    "            if d.get(key) > max:\n",
    "                max = d.get(key)\n",
    "                value = []\n",
    "                value.append(key)\n",
    "            elif d.get(key) == max:\n",
    "                '''se ci sono più massimi li metto tutti in una lista dalla quale ne\n",
    "                    selezionerò uno randomicamente\n",
    "                '''\n",
    "                value.append(key)\n",
    "        return rn.choice(value)\n",
    "        \n",
    "    def SameClassification(self, examples): \n",
    "        if (len(examples) == 0):\n",
    "            return False\n",
    "        d = self.CreateDictionary(examples)\n",
    "        if (len(d.keys()) == 1):\n",
    "            return list(d.keys())[0]\n",
    "        return False\n",
    "    def CreateDictionary(self, examples):\n",
    "        examples = examples[self.outputstring].tolist()\n",
    "        d = {}\n",
    "        for i in range(len(examples)):\n",
    "            if d.get(examples[i]) is None:\n",
    "                d[examples[i]] = 1\n",
    "            else:\n",
    "                d[examples[i]] += 1\n",
    "        return d\n",
    "\n",
    "    def Values1(self, attribute, dictionary): \n",
    "        return list(dictionary.get(attribute))\n",
    "    \n",
    "    def Values2(self, attribute, examples):\n",
    "        return examples[attribute].unique()\n",
    "\n",
    "    def Examples(self, attribute, examples, value):\n",
    "        exs = examples.loc[examples[attribute] == value]\n",
    "        return exs\n",
    "    def PopListValue(self, lista, value):\n",
    "        if value not in lista:\n",
    "            return None\n",
    "        lista.remove(value)\n",
    "        return lista \n",
    "    def PrintDecisionTree(self, count):\n",
    "        print(\" \"*count, self.label)\n",
    "        for elem in self.nodes:\n",
    "            if type(elem[1]) is not DecisionTree:\n",
    "                print(\"    \"*(count+1), elem[0], \" --> \", elem[1])\n",
    "            else:\n",
    "                elem[1].PrintDecisionTree(count+1)\n",
    "\n",
    "    def Prediction(self, input):\n",
    "        for i in range(len(self.nodes)):\n",
    "            x = input[self.label].values[0]\n",
    "            if type(self.nodes[i][0]) is not str and math.isnan(float(self.nodes[i][0])):\n",
    "                if type(x) is not str and math.isnan(float(x)):\n",
    "                    if type(self.nodes[i][1]) is DecisionTree:\n",
    "                        return self.nodes[i][1].Prediction(input)\n",
    "                    return self.nodes[i][1]\n",
    "            elif self.nodes[i][0] == x:\n",
    "                if type(self.nodes[i][1]) is DecisionTree:\n",
    "                    return self.nodes[i][1].Prediction(input)\n",
    "                return self.nodes[i][1]\n",
    "        return None\n",
    "\n",
    "class GradientDescent:\n",
    "    def __init__(self, learningrate, epochs, parameters_length, output_string, batch_size): \n",
    "        self.learningrate = learningrate\n",
    "        self.epochs = epochs\n",
    "        self.parameters_length = parameters_length\n",
    "        self.parameters = None\n",
    "        self.output_string = output_string\n",
    "        self.len_totale = None\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def SGD(self, training_set):\n",
    "        \n",
    "        self.parameters = np.ones(self.parameters_length)\n",
    "        \n",
    "        input_totale = training_set.loc[:len(training_set), training_set.columns != self.output_string]\n",
    "        output_totale = training_set.loc[:len(training_set), training_set.columns == self.output_string]\n",
    "        self.len_totale = len(training_set)\n",
    "\n",
    "        error = self.MSE(self.prediction(input_totale), output_totale)\n",
    "\n",
    "        for i in range(self.epochs):\n",
    "            \n",
    "            batch = training_set.sample(n=self.batch_size)\n",
    "            input = batch.loc[:, training_set.columns != self.output_string]\n",
    "            output = batch.loc[:, training_set.columns == self.output_string]\n",
    "            \n",
    "            self.learningrate = 10000/((10000+i))\n",
    "            gradient = self.gradient(input, output)\n",
    "            self.SGD_aux(gradient)\n",
    "            \n",
    "            if (not (i%10000)):\n",
    "                print(\"Epoca: \\t\", i)\n",
    "                print(\"MSE: \", self.MSE(self.prediction(input_totale), output_totale))\n",
    "                \n",
    "        return self.parameters\n",
    "\n",
    "    def SGD_aux(self, gradient):\n",
    "        for f in range(self.parameters_length):\n",
    "            self.parameters[f] = self.parameters[f]-(self.learningrate*gradient[f][0])\n",
    "\n",
    "    def gradient(self, input, output): \n",
    "        output = np.array(output)\n",
    "        prediction = self.prediction(input)\n",
    "        error = prediction - output\n",
    "        coefficient = input.shape[0]/self.len_totale\n",
    "        return np.dot(coefficient*input.T, error)\n",
    "\n",
    "    def prediction(self, input):\n",
    "        return np.dot(input, self.parameters)\n",
    "\n",
    "    def MSE(self, prediction, output):\n",
    "        output = np.array(output)\n",
    "        return np.mean((output - prediction)**2)\n",
    "    def RMSE(self, prediction, output):\n",
    "        return math.sqrt(np.mean((output - prediction)**2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "2fa37c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoca: \t 0\n",
      "MSE:  94.24147309169841\n",
      "Epoca: \t 10000\n",
      "MSE:  0.22890763703815303\n",
      "Epoca: \t 20000\n",
      "MSE:  0.19356253868187628\n",
      "Epoca: \t 30000\n",
      "MSE:  0.1827545283193402\n",
      "Epoca: \t 40000\n",
      "MSE:  0.17702964846606886\n",
      "Epoca: \t 50000\n",
      "MSE:  0.17361941235674144\n",
      "Epoca: \t 60000\n",
      "MSE:  0.17074111877952053\n",
      "Epoca: \t 70000\n",
      "MSE:  0.16841270253935714\n",
      "Epoca: \t 80000\n",
      "MSE:  0.16661877261073582\n",
      "Epoca: \t 90000\n",
      "MSE:  0.16500713172085618\n",
      "Epoca: \t 100000\n",
      "MSE:  0.1635470102406828\n",
      "Epoca: \t 110000\n",
      "MSE:  0.16243225631595523\n",
      "Epoca: \t 120000\n",
      "MSE:  0.16138431931451483\n",
      "Epoca: \t 130000\n",
      "MSE:  0.16056409287257625\n",
      "Epoca: \t 140000\n",
      "MSE:  0.15966304366315728\n",
      "Epoca: \t 150000\n",
      "MSE:  0.15860629403363835\n",
      "Epoca: \t 160000\n",
      "MSE:  0.1580507687271559\n",
      "Epoca: \t 170000\n",
      "MSE:  0.1573484501682887\n",
      "Epoca: \t 180000\n",
      "MSE:  0.1564470653662937\n",
      "Epoca: \t 190000\n",
      "MSE:  0.15579435039827638\n",
      "Epoca: \t 200000\n",
      "MSE:  0.15521490057351608\n",
      "Epoca: \t 210000\n",
      "MSE:  0.154598423366599\n",
      "Epoca: \t 220000\n",
      "MSE:  0.15432764047659145\n",
      "Epoca: \t 230000\n",
      "MSE:  0.15390727520290415\n",
      "Epoca: \t 240000\n",
      "MSE:  0.1533431217799046\n",
      "Epoca: \t 250000\n",
      "MSE:  0.15289120926250738\n",
      "Epoca: \t 260000\n",
      "MSE:  0.15249554214459832\n",
      "Epoca: \t 270000\n",
      "MSE:  0.1519911245476993\n",
      "Epoca: \t 280000\n",
      "MSE:  0.15165160337802397\n",
      "Epoca: \t 290000\n",
      "MSE:  0.15130333361061657\n",
      "Epoca: \t 300000\n",
      "MSE:  0.1507614188460321\n",
      "Epoca: \t 310000\n",
      "MSE:  0.15046727823289982\n",
      "Epoca: \t 320000\n",
      "MSE:  0.15009630375690347\n",
      "Epoca: \t 330000\n",
      "MSE:  0.1497093189485657\n",
      "Epoca: \t 340000\n",
      "MSE:  0.14941649504209947\n",
      "Epoca: \t 350000\n",
      "MSE:  0.14922086624245137\n",
      "Epoca: \t 360000\n",
      "MSE:  0.14874019173394942\n",
      "Epoca: \t 370000\n",
      "MSE:  0.14842435607808954\n",
      "Epoca: \t 380000\n",
      "MSE:  0.14820814181276798\n",
      "Epoca: \t 390000\n",
      "MSE:  0.1480289732352563\n",
      "Epoca: \t 400000\n",
      "MSE:  0.14769974343801973\n",
      "Epoca: \t 410000\n",
      "MSE:  0.14744523536647686\n",
      "Epoca: \t 420000\n",
      "MSE:  0.14721164700403006\n",
      "Epoca: \t 430000\n",
      "MSE:  0.14686968877181894\n",
      "Epoca: \t 440000\n",
      "MSE:  0.14660566855519264\n",
      "Epoca: \t 450000\n",
      "MSE:  0.14635703597747604\n",
      "Epoca: \t 460000\n",
      "MSE:  0.14625090770490104\n",
      "Epoca: \t 470000\n",
      "MSE:  0.14600561134640672\n",
      "Epoca: \t 480000\n",
      "MSE:  0.14587110576322948\n",
      "Epoca: \t 490000\n",
      "MSE:  0.14565402282773873\n",
      "Epoca: \t 500000\n",
      "MSE:  0.14550661518031133\n",
      "Epoca: \t 510000\n",
      "MSE:  0.14534772221341624\n",
      "Epoca: \t 520000\n",
      "MSE:  0.1450546032336407\n",
      "Epoca: \t 530000\n",
      "MSE:  0.14484572414953345\n",
      "Epoca: \t 540000\n",
      "MSE:  0.14463879541614252\n",
      "Epoca: \t 550000\n",
      "MSE:  0.1445231349926078\n",
      "Epoca: \t 560000\n",
      "MSE:  0.1443498665784602\n",
      "Epoca: \t 570000\n",
      "MSE:  0.144149354179189\n",
      "Epoca: \t 580000\n",
      "MSE:  0.1440283419071921\n",
      "Epoca: \t 590000\n",
      "MSE:  0.14380203194870955\n",
      "Epoca: \t 600000\n",
      "MSE:  0.14362016878466202\n",
      "Epoca: \t 610000\n",
      "MSE:  0.14349756786160928\n",
      "Epoca: \t 620000\n",
      "MSE:  0.14340952995283945\n",
      "Epoca: \t 630000\n",
      "MSE:  0.14326767486914546\n",
      "Epoca: \t 640000\n",
      "MSE:  0.14311004113835252\n",
      "Epoca: \t 650000\n",
      "MSE:  0.14302565439606696\n",
      "Epoca: \t 660000\n",
      "MSE:  0.1428890323154571\n",
      "Epoca: \t 670000\n",
      "MSE:  0.14274333746246437\n",
      "Epoca: \t 680000\n",
      "MSE:  0.14258077383447473\n",
      "Epoca: \t 690000\n",
      "MSE:  0.1423894162451343\n",
      "Epoca: \t 700000\n",
      "MSE:  0.14225733201235632\n",
      "Epoca: \t 710000\n",
      "MSE:  0.14212305164862168\n",
      "Epoca: \t 720000\n",
      "MSE:  0.14202834221468763\n",
      "Epoca: \t 730000\n",
      "MSE:  0.14183693619773322\n",
      "Epoca: \t 740000\n",
      "MSE:  0.14176961237432148\n",
      "Epoca: \t 750000\n",
      "MSE:  0.14166017764153224\n",
      "Epoca: \t 760000\n",
      "MSE:  0.14147950536604378\n",
      "Epoca: \t 770000\n",
      "MSE:  0.14133398969002298\n",
      "Epoca: \t 780000\n",
      "MSE:  0.1412274566463361\n",
      "Epoca: \t 790000\n",
      "MSE:  0.14113117412834522\n",
      "Epoca: \t 800000\n",
      "MSE:  0.1410574957945233\n",
      "Epoca: \t 810000\n",
      "MSE:  0.1409686074131577\n",
      "Epoca: \t 820000\n",
      "MSE:  0.14086679078650075\n",
      "Epoca: \t 830000\n",
      "MSE:  0.14077948254142497\n",
      "Epoca: \t 840000\n",
      "MSE:  0.1406820879098927\n",
      "Epoca: \t 850000\n",
      "MSE:  0.14058209811876618\n",
      "Epoca: \t 860000\n",
      "MSE:  0.14047792539657816\n",
      "Epoca: \t 870000\n",
      "MSE:  0.1403711746576924\n",
      "Epoca: \t 880000\n",
      "MSE:  0.14029166498293377\n",
      "Epoca: \t 890000\n",
      "MSE:  0.1401803111170767\n",
      "Epoca: \t 900000\n",
      "MSE:  0.14005958877617186\n",
      "Epoca: \t 910000\n",
      "MSE:  0.13995100899142804\n",
      "Epoca: \t 920000\n",
      "MSE:  0.13984928873722505\n",
      "Epoca: \t 930000\n",
      "MSE:  0.1397525594816454\n",
      "Epoca: \t 940000\n",
      "MSE:  0.13965300398068878\n",
      "Epoca: \t 950000\n",
      "MSE:  0.13962518676253097\n",
      "Epoca: \t 960000\n",
      "MSE:  0.13949928187365246\n",
      "Epoca: \t 970000\n",
      "MSE:  0.13940592089799947\n",
      "Epoca: \t 980000\n",
      "MSE:  0.13935620907591018\n",
      "Epoca: \t 990000\n",
      "MSE:  0.1393038057062776\n",
      "1509.4572141170502 [-9.12004830e-01  4.10421134e-01  2.58033505e-01 -1.40260343e-01\n",
      "  2.67824855e-02 -6.37396030e-02  9.31195061e-02  1.33209880e-01\n",
      "  1.99903004e-04 -1.00265369e-01 -1.93927924e-03  3.26744593e-01\n",
      "  2.03808325e-01  5.88508114e-01  5.26404545e-01  2.50615200e-01\n",
      "  4.72528726e-01  3.03618404e-01  3.19012892e-01  9.15623244e-01\n",
      "  8.58287604e-01  9.59354865e-02  5.13490244e-01  2.47298054e-01\n",
      "  6.51612147e-01  7.83480582e-01  9.76429943e-01  4.05881173e-01\n",
      "  5.38094151e-01]\n"
     ]
    }
   ],
   "source": [
    "gd = GradientDescent(1, 1000000, dataset_training.shape[1]-1, 'Weight', 1)\n",
    "\n",
    "start = time.time()\n",
    "pesi = gd.SGD(dataset_training)\n",
    "end = time.time()\n",
    "print(end-start, pesi)\n",
    "\n",
    "#'''sklearn'''\n",
    "#X = dataset_training.loc[:, dataset_training.columns != 'Weight']\n",
    "#Y = dataset_training.loc[:, dataset_training.columns == 'Weight']\n",
    "#\n",
    "#clf = SGDClassifier(max_iter = 1, tol=1e-3)\n",
    "#clf.fit(X, Y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "21aa53e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  0.13194763756535088\n",
      "RMSE:  0.36324597391485414\n",
      "MAE:  0.2903156196847688\n",
      "MSE_sklearn:  0.07227572911243803\n",
      "RMSE_sklearn:  0.2688414572056141\n",
      "MAE_sklearn:  0.2184483581987949\n"
     ]
    }
   ],
   "source": [
    "X_test = dataset_test.loc[:, dataset_test.columns != 'Weight']\n",
    "X_test = np.array(X_test)\n",
    "Y_effettivo = np.array(dataset_test['Weight'])\n",
    "\n",
    "X = dataset_training.loc[:, dataset_training.columns != 'Weight']\n",
    "Y = np.array(dataset_training.loc[:, dataset_training.columns == 'Weight'])\n",
    "\n",
    "Y_predetto = np.array(np.matmul(X_test, pesi))\n",
    "Y_predetto_training = np.array(np.matmul(X, pesi))\n",
    "\n",
    "map = Map()\n",
    "\n",
    "print(\"MSE: \", map.MSE(Y, Y_predetto_training))\n",
    "print(\"RMSE: \", map.RMSE(Y, Y_predetto_training))\n",
    "print(\"MAE: \", map.MAE(Y, Y_predetto_training))\n",
    "\n",
    "\n",
    "#print(\"MSE: \", map.MSE(Y_effettivo, Y_predetto))\n",
    "#print(\"RMSE: \", map.RMSE(Y_effettivo, Y_predetto))\n",
    "#print(\"MAE: \", map.MAE(Y_effettivo, Y_predetto))\n",
    "#\n",
    "#'''sklearn'''\n",
    "\n",
    "#\n",
    "#reg = LinearRegression().fit(X, Y)\n",
    "#scikit_predict = reg.predict(X_test)\n",
    "#print(\"MSE_sklearn: \", map.MSE(Y_effettivo, scikit_predict))\n",
    "#print(\"RMSE_sklearn: \", map.RMSE(Y_effettivo, scikit_predict))\n",
    "#print(\"MAE_sklearn: \", map.MAE(Y_effettivo, scikit_predict))\n",
    "\n",
    "reg = LinearRegression().fit(X, Y)\n",
    "scikit_predict = reg.predict(X)\n",
    "\n",
    "print(\"MSE_sklearn: \", map.MSE(Y_effettivo, scikit_predict))\n",
    "print(\"RMSE_sklearn: \", map.RMSE(Y_effettivo, scikit_predict))\n",
    "print(\"MAE_sklearn: \", map.MAE(Y_effettivo, scikit_predict))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
