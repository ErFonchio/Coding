{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random as rn\n",
    "import pandas as pd\n",
    "from library import Map, GradientDescent\n",
    "import time\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from random import randrange\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yq/gyf11wn125qgkzlcv24mbp3w0000gn/T/ipykernel_99508/4154594298.py:9: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  dataset.replace(to_replace=(\"Obesity_Type_I\", \"Obesity_Type_II\", \"Obesity_Type_III\"), value=1, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "'''Downloading dataset'''\n",
    "dataset = pd.read_csv(\"/Users/alessandrococcia/Downloads/ObesityDataSet.csv\")\n",
    "\n",
    "'''Dataset sampling'''\n",
    "dataset = dataset.sample(frac=1, ignore_index=True) #shuffle sample in the training set\n",
    "\n",
    "'''mapping delle stringhe'''\n",
    "dataset.replace(to_replace=(\"Insufficient_Weight\", \"Normal_Weight\", \"Overweight_Level_I\", \"Overweight_Level_II\"), value=0, inplace=True)\n",
    "dataset.replace(to_replace=(\"Obesity_Type_I\", \"Obesity_Type_II\", \"Obesity_Type_III\"), value=1, inplace=True)\n",
    "dataset = pd.get_dummies(dataset, drop_first=True, dtype=float)\n",
    "\n",
    "'''normalization'''\n",
    "dataset = (dataset-dataset.mean())/dataset.std()\n",
    "\n",
    "'''Inserimento colonna di bias'''\n",
    "dataset.insert(0, \"Bias\", np.ones(len(dataset)), True) #Bias row\n",
    "\n",
    "\n",
    "TRAIN_TEST_SPLIT_PERCENTAGE = 0.9\n",
    "dataset_training = dataset[:int(len(dataset) * TRAIN_TEST_SPLIT_PERCENTAGE)]\n",
    "dataset_test = dataset[int(len(dataset) * TRAIN_TEST_SPLIT_PERCENTAGE):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Map:\n",
    "    def __init__(self):\n",
    "        self.count = 0\n",
    "        self.count_dataset = {}\n",
    "        self.d = {}\n",
    "        self.dataset_dictionary = {}\n",
    "        self.matrice = None\n",
    "        self.dataset = None\n",
    "        len = None\n",
    "    \n",
    "    def mappingElement(self, string):\n",
    "        if type(string) is str:\n",
    "            if self.d.get(string) is not None:\n",
    "                return self.d.get(string)\n",
    "            else:\n",
    "                self.count = self.count+1\n",
    "                self.d[string] = self.count\n",
    "                return self.count\n",
    "        return float(string)\n",
    "    \n",
    "    def mappingMatrix(self, matrice):\n",
    "        self.matrice = None\n",
    "        self.matrice = np.array(matrice)\n",
    "        len = self.matrice.shape\n",
    "        for g in range(len[0]):\n",
    "            for j in range(len[1]):\n",
    "                self.matrice[g][j] = self.mappingElement(self.matrice[g][j])\n",
    "        return np.float64(self.matrice.copy())\n",
    "    \n",
    "    def mappingElementDataset(self, value, colonna):\n",
    "        if type(value) is str:\n",
    "            if self.dataset_dictionary[colonna].get(value) is not None:\n",
    "                return self.dataset_dictionary[colonna].get(value)\n",
    "            else:\n",
    "                if self.count_dataset.get(colonna) is None:\n",
    "                    self.count_dataset[colonna] = 0\n",
    "                    self.dataset_dictionary[colonna][value] = self.count_dataset[colonna]\n",
    "                    return self.count_dataset[colonna]\n",
    "                self.count_dataset[colonna] += 1\n",
    "                self.dataset_dictionary[colonna][value] = self.count_dataset[colonna]\n",
    "                return self.count_dataset[colonna]\n",
    "        return value\n",
    "    \n",
    "    def mappingDataset(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        colonne = dataset.columns\n",
    "        length = len(dataset)\n",
    "        for column in colonne:\n",
    "            self.dataset_dictionary[column] = {}\n",
    "            for index in range(length):\n",
    "                dataset.at[index, column] = self.mappingElementDataset(dataset.at[index, column], column)\n",
    "        return self.dataset\n",
    "\n",
    "    def MSE(self, a, b):\n",
    "        return np.mean((np.square(a - b)))\n",
    "    def RMSE(self, a, b):\n",
    "        return math.sqrt(np.mean(np.square(a - b)))\n",
    "    def MAE(self, a, b):\n",
    "        return np.mean(abs(a-b))\n",
    "    \n",
    "class DecisionTree:\n",
    "    def __init__(self, outputstring, positive, negative): \n",
    "        self.outputstring = outputstring\n",
    "        self.positive = positive\n",
    "        self.negative = negative\n",
    "        self.nodes = []\n",
    "        self.label = None\n",
    "        self.root = False\n",
    "\n",
    "    def LearnDecisionTree(self, examples, attributes, parent_examples, column_values):\n",
    "        self.root = True\n",
    "        self.LearnDecisionTreeAux_(examples, attributes, parent_examples, column_values)\n",
    "\n",
    "    def LearnDecisionTreeAux_(self, examples, attributes, parent_examples, column_values):\n",
    "        same_classification = self.SameClassification(examples)    \n",
    "        \n",
    "        if len(examples.loc[:, examples.columns != self.outputstring]) == 0:\n",
    "            return self.PluralityValue(parent_examples)\n",
    "        elif same_classification is not False:\n",
    "            return same_classification\n",
    "        elif len(attributes) == 0: \n",
    "            return self.PluralityValue(examples)\n",
    "        else:\n",
    "            '''Seleziono l'attributo migliore'''\n",
    "            bestattribute = self.Importance(attributes, examples)\n",
    "            self.label = bestattribute #Seleziono l'attributo migliore\n",
    "            \n",
    "            for value in self.Values1(bestattribute, column_values):\n",
    "                remainingexamples = self.Examples(bestattribute, examples, value)\n",
    "                tree = DecisionTree(self.outputstring, self.positive, self.negative)\n",
    "                attributes_left = self.PopListValue(attributes.copy(), bestattribute)                \n",
    "                subtree = tree.LearnDecisionTreeAux_(remainingexamples.loc[:, remainingexamples.columns != bestattribute], attributes_left, examples, column_values)\n",
    "                self.nodes.append((value, subtree))\n",
    "        return self\n",
    "        \n",
    "    def Importance(self, attributes, examples): \n",
    "        max = -1\n",
    "        ret = None\n",
    "        for a in attributes:\n",
    "            loc = self.Gain(examples, a)\n",
    "            if loc > max:\n",
    "                max = loc\n",
    "                ret = a\n",
    "        return ret\n",
    "        \n",
    "    def B(self, q):\n",
    "        if q == 1 or q == 0:\n",
    "            return 0\n",
    "        return -(q*math.log2(q)+(1-q)*math.log2(1-q))\n",
    "    \n",
    "    def Remainder(self, examples, attribute, p, n):\n",
    "        sum = 0\n",
    "        for v in self.Values2(attribute, examples):\n",
    "            if type(v) is not str and math.isnan(float(v)):\n",
    "                pk = len(examples.loc[(examples[self.outputstring] == self.positive) & (examples[attribute].isnull())])\n",
    "                nk = len(examples.loc[(examples[self.outputstring] == self.negative) & (examples[attribute].isnull())])\n",
    "            else:\n",
    "                pk = len(examples.loc[(examples[self.outputstring] == self.positive) & (examples[attribute] == v)])\n",
    "                nk = len(examples.loc[(examples[self.outputstring] == self.negative) & ((examples[attribute] == v))])\n",
    "            \n",
    "            partial = (pk+nk)*self.B(pk/(pk+nk))\n",
    "            sum += partial\n",
    "        return (1/(p+n))*sum\n",
    "    \n",
    "    def Gain(self, examples, attribute):\n",
    "        p = len(examples.loc[examples[self.outputstring] == self.positive])\n",
    "        n = len(examples.loc[examples[self.outputstring] == self.negative])\n",
    "        b = self.B(p/(p+n))\n",
    "        r = self.Remainder(examples, attribute, p, n)\n",
    "        return (b-r)\n",
    "        \n",
    "    def PluralityValue(self, parent_examples): \n",
    "        '''Selects the most common ouput value among a set of examples, breaking ties randomly'''\n",
    "        value, max = [], 0\n",
    "        d = self.CreateDictionary(parent_examples)\n",
    "        for key in d.keys():\n",
    "            if d.get(key) > max:\n",
    "                max = d.get(key)\n",
    "                value = []\n",
    "                value.append(key)\n",
    "            elif d.get(key) == max:\n",
    "                '''se ci sono più massimi li metto tutti in una lista dalla quale ne\n",
    "                    selezionerò uno randomicamente\n",
    "                '''\n",
    "                value.append(key)\n",
    "        return rn.choice(value)\n",
    "        \n",
    "    def SameClassification(self, examples): \n",
    "        if (len(examples) == 0):\n",
    "            return False\n",
    "        d = self.CreateDictionary(examples)\n",
    "        if (len(d.keys()) == 1):\n",
    "            return list(d.keys())[0]\n",
    "        return False\n",
    "    def CreateDictionary(self, examples):\n",
    "        examples = examples[self.outputstring].tolist()\n",
    "        d = {}\n",
    "        for i in range(len(examples)):\n",
    "            if d.get(examples[i]) is None:\n",
    "                d[examples[i]] = 1\n",
    "            else:\n",
    "                d[examples[i]] += 1\n",
    "        return d\n",
    "\n",
    "    def Values1(self, attribute, dictionary): \n",
    "        return list(dictionary.get(attribute))\n",
    "    \n",
    "    def Values2(self, attribute, examples):\n",
    "        return examples[attribute].unique()\n",
    "\n",
    "    def Examples(self, attribute, examples, value):\n",
    "        exs = examples.loc[examples[attribute] == value]\n",
    "        return exs\n",
    "    def PopListValue(self, lista, value):\n",
    "        if value not in lista:\n",
    "            return None\n",
    "        lista.remove(value)\n",
    "        return lista \n",
    "    def PrintDecisionTree(self, count):\n",
    "        print(\" \"*count, self.label)\n",
    "        for elem in self.nodes:\n",
    "            if type(elem[1]) is not DecisionTree:\n",
    "                print(\"    \"*(count+1), elem[0], \" --> \", elem[1])\n",
    "            else:\n",
    "                elem[1].PrintDecisionTree(count+1)\n",
    "\n",
    "    def Prediction(self, input):\n",
    "        for i in range(len(self.nodes)):\n",
    "            x = input[self.label].values[0]\n",
    "            if type(self.nodes[i][0]) is not str and math.isnan(float(self.nodes[i][0])):\n",
    "                if type(x) is not str and math.isnan(float(x)):\n",
    "                    if type(self.nodes[i][1]) is DecisionTree:\n",
    "                        return self.nodes[i][1].Prediction(input)\n",
    "                    return self.nodes[i][1]\n",
    "            elif self.nodes[i][0] == x:\n",
    "                if type(self.nodes[i][1]) is DecisionTree:\n",
    "                    return self.nodes[i][1].Prediction(input)\n",
    "                return self.nodes[i][1]\n",
    "        return None\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, learningrate, epochs, parameters_length, output_string, batch_size, threshold, output_values, output_totale): \n",
    "        self.learningrate = learningrate\n",
    "        self.epochs = epochs\n",
    "        self.parameters_length = parameters_length\n",
    "        self.parameters = None\n",
    "        self.len_totale = None\n",
    "        self.batch_size = batch_size\n",
    "        self.threshold = threshold\n",
    "        self.output_values = output_values\n",
    "        self.output_string = output_string\n",
    "        self.output_totale = output_totale\n",
    "\n",
    "    def Discesa(self, training_set):\n",
    "        \n",
    "        '''Inizializzo i pesi ad 1'''\n",
    "        self.parameters = np.ones(self.parameters_length)\n",
    "        '''Su input e output totale calcolerò ad ogni n-esima epoca l'errore'''\n",
    "        input_totale = training_set.loc[:len(training_set), training_set.columns != self.output_string]\n",
    "        output_totale = training_set.loc[:len(training_set), training_set.columns == self.output_string]\n",
    "        self.len_totale = len(training_set)\n",
    "\n",
    "        '''ciclo for per le epoche'''\n",
    "        '''Batch == input totale: non devo randomizzare'''\n",
    "        if self.batch_size == self.len_totale:\n",
    "            input = input_totale\n",
    "            output = output_totale\n",
    "            for i in range(self.epochs):\n",
    "                update = self.WeightsUpdate(input, output)\n",
    "                self.Discesa_aux(update)\n",
    "                if (not (i%10000)):\n",
    "                    print(\"Epoca: \\t\", i)\n",
    "                    #self.classificazione(self.output_totale, input)\n",
    "        \n",
    "        else:\n",
    "            '''uso delle minibatch che devono avere i sample randomizzati ad ogni iterazione'''\n",
    "            for i in range(self.epochs):\n",
    "                batch = training_set.sample(n=self.batch_size)\n",
    "                input = batch.loc[:, training_set.columns != self.output_string]\n",
    "                output = batch.loc[:, training_set.columns == self.output_string]\n",
    "                \n",
    "                update = self.WeightsUpdate(input, output)\n",
    "                self.Discesa_aux(update)\n",
    "                \n",
    "                if (not (i%10000)):\n",
    "                    print(\"Epoca: \\t\", i)\n",
    "                    #self.classificazione(self.output_totale, input)\n",
    "                \n",
    "        return self.parameters\n",
    "\n",
    "    def Discesa_aux(self, update):\n",
    "        self.parameters -= self.learningrate*update\n",
    "\n",
    "    def WeightsUpdate(self, input, output): \n",
    "        hw = self.Sigmoid(input)\n",
    "        output = np.array(output.T)[0]\n",
    "        error = np.array(hw-output)\n",
    "        partial = error * hw * (1 - hw)\n",
    "        return np.dot(input.T, partial.T).T/input.shape[0]\n",
    "    \n",
    "    def Sigmoid(self, input):\n",
    "        upper_value = self.output_values[0]\n",
    "        lower_value = self.output_values[1]\n",
    "        array = 1/(1+np.exp(-input@self.parameters))\n",
    "        #print(array)\n",
    "        array[array > self.threshold] = upper_value\n",
    "        array[array <= self.threshold] = lower_value\n",
    "        return array\n",
    "        \n",
    "    def prediction(self, input):\n",
    "        return np.dot(input, self.parameters)\n",
    "\n",
    "    def MSE(self, prediction, output):\n",
    "        output = np.array(output)\n",
    "        return np.mean((output - prediction)**2)\n",
    "    def RMSE(self, prediction, output):\n",
    "        return math.sqrt(np.mean((output - prediction)**2))\n",
    "    \n",
    "    def classificazione(self, output, input) -> None: \n",
    "        prediction = self.LogisticFunction(input)\n",
    "        print(prediction)\n",
    "        count = 0\n",
    "        for i in range(len(prediction)):\n",
    "            print(i)\n",
    "            if prediction[i] == output[i]:\n",
    "                count += 1\n",
    "\n",
    "        accuratezza = count/len(output)\n",
    "        print(accuratezza)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoca: \t 0\n"
     ]
    }
   ],
   "source": [
    "output_string = 'NObeyesdad'\n",
    "input_totale = dataset_training.loc[:len(dataset_training), dataset_training.columns != output_string]\n",
    "output_totale = dataset_training.loc[:len(dataset_training), dataset_training.columns == output_string]\n",
    "\n",
    "'''Values to determine threshold'''\n",
    "positive_value, negative_value = max(output_totale['NObeyesdad'].unique()), min(output_totale['NObeyesdad'].unique())\n",
    "threshold = 0.5\n",
    "\n",
    "lr = LogisticRegression(learningrate=2e-2, epochs=10000, parameters_length=dataset_training.shape[1]-1, \n",
    "                        output_string=output_string, batch_size=dataset_training.shape[0], threshold=threshold, \n",
    "                        output_values=(positive_value, negative_value), output_totale=output_totale)\n",
    "lr.parameters = np.ones(dataset_training.shape[1]-1)\n",
    "pesi = lr.Discesa(dataset_training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 212\n"
     ]
    }
   ],
   "source": [
    "X_test = np.array(dataset_test.loc[:, dataset_test.columns != 'NObeyesdad'])\n",
    "Y_effettivo = np.array(dataset_test['NObeyesdad'])\n",
    "\n",
    "def LogisticFunction2(input, pesi, threshold, out_values):\n",
    "        upper_value = out_values[0]\n",
    "        lower_value = out_values[1]\n",
    "        array = 1/(1+np.exp(-np.dot(pesi, input.T)))\n",
    "        array[array > threshold] = upper_value\n",
    "        array[array <= threshold] = lower_value\n",
    "        return array\n",
    "\n",
    "prediction = LogisticFunction2(X_test, pesi, lr.threshold, lr.output_values)\n",
    "count = 0\n",
    "for i in range(len(prediction)):\n",
    "    if prediction[i] == Y_effettivo[i]:\n",
    "        count += 1\n",
    "\n",
    "accuratezza = count/len(Y_effettivo)\n",
    "print(count, len(Y_effettivo))\n",
    "#lr.classificazione(Y_effettivo, X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  22.990714242578033\n",
      "RMSE:  4.7948633184459\n",
      "MAE:  3.4733108627973874\n",
      "MSE_sklearn:  1.7455929079487433\n",
      "RMSE_sklearn:  1.3212088812707639\n",
      "MAE_sklearn:  1.0769804733649415\n",
      "MSE_sklearn SGD:  0.18581967910428507\n",
      "RMSE_sklearn SGD:  0.43106806783185075\n",
      "MAE_sklearn SGD:  0.33914236933599606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "X_test = np.array(dataset_test.loc[:, dataset_test.columns != 'Weight'])\n",
    "Y_effettivo = np.array(dataset_test['Weight'])\n",
    "Y_predetto = np.array(np.matmul(X_test, pesi))\n",
    "\n",
    "\n",
    "map = Map()\n",
    "print(\"MSE: \", map.MSE(Y_effettivo, Y_predetto))\n",
    "print(\"RMSE: \", map.RMSE(Y_effettivo, Y_predetto))\n",
    "print(\"MAE: \", map.MAE(Y_effettivo, Y_predetto))\n",
    "\n",
    "'''sklearn'''\n",
    "\n",
    "X = np.array(dataset_training.loc[:, dataset_training.columns != 'Weight'])\n",
    "Y = np.array(dataset_training.loc[:, dataset_training.columns == 'Weight'])\n",
    "\n",
    "reg = LinearRegression().fit(X, Y)\n",
    "scikit_predict = np.array(reg.predict(X_test))\n",
    "\n",
    "print(\"MSE_sklearn: \", map.MSE(Y_effettivo, scikit_predict))\n",
    "print(\"RMSE_sklearn: \", map.RMSE(Y_effettivo, scikit_predict))\n",
    "print(\"MAE_sklearn: \", map.MAE(Y_effettivo, scikit_predict))\n",
    "\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "# X_test e Y_effettivo sono i dati di test\n",
    "\n",
    "# Definisci il regressore SGD\n",
    "sgd_regressor = SGDRegressor(max_iter=10000, alpha=0.001, random_state=42)\n",
    "\n",
    "# Addestra il modello\n",
    "sgd_regressor.fit(X, Y)\n",
    "\n",
    "# Effettua previsioni\n",
    "Y_predetto_sgd = np.array(sgd_regressor.predict(X_test))\n",
    "\n",
    "# Calcola le metriche di valutazione\n",
    "\n",
    "# Stampare le metriche di valutazione\n",
    "print(\"MSE_sklearn SGD: \", map.MSE(Y_effettivo, Y_predetto_sgd))\n",
    "print(\"RMSE_sklearn SGD: \", map.RMSE(Y_effettivo, Y_predetto_sgd))\n",
    "print(\"MAE_sklearn SGD: \", map.MAE(Y_effettivo, Y_predetto_sgd))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
