{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d94ea7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random as rn\n",
    "import pandas as pd\n",
    "from library import Map, GradientDescent\n",
    "import time\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from random import randrange\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51a9d4e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Bias       Age    Height    Weight      FCVC       NCP      CH2O  \\\n",
      "0      1.0 -0.522001 -0.875382 -0.862354 -0.784833  0.404057 -0.013070   \n",
      "1      1.0 -0.522001 -1.947138 -1.167800  1.088084  0.404057  1.618375   \n",
      "2      1.0 -0.206840  1.053779 -0.366003 -0.784833  0.404057 -0.013070   \n",
      "3      1.0  0.423481  1.053779  0.015805  1.088084  0.404057 -0.013070   \n",
      "4      1.0 -0.364420  0.839428  0.122711 -0.784833 -2.166509 -0.013070   \n",
      "...    ...       ...       ...       ...       ...       ...       ...   \n",
      "2106   1.0 -0.525650  0.097022  1.711358  1.088084  0.404057 -0.456597   \n",
      "2107   1.0 -0.367108  0.502725  1.800488  1.088084  0.404057 -0.004701   \n",
      "2108   1.0 -0.281843  0.541544  1.798442  1.088084  0.404057  0.075343   \n",
      "2109   1.0  0.007774  0.404831  1.785357  1.088084  0.404057  1.377474   \n",
      "2110   1.0 -0.102095  0.398250  1.790167  1.088084  0.404057  1.395704   \n",
      "\n",
      "           FAF       TUE  Gender_Male  ...  MTRANS_Bike  MTRANS_Motorbike  \\\n",
      "0    -1.187758  0.561864    -1.011674  ...    -0.057666         -0.072358   \n",
      "1     2.339196 -1.080369    -1.011674  ...    -0.057666         -0.072358   \n",
      "2     1.163545  0.561864     0.987992  ...    -0.057666         -0.072358   \n",
      "3     1.163545 -1.080369     0.987992  ...    -0.057666         -0.072358   \n",
      "4    -1.187758 -1.080369     0.987992  ...    -0.057666         -0.072358   \n",
      "...        ...       ...          ...  ...          ...               ...   \n",
      "2106  0.782950  0.407899    -1.011674  ...    -0.057666         -0.072358   \n",
      "2107  0.389249 -0.096228    -1.011674  ...    -0.057666         -0.072358   \n",
      "2108  0.474859 -0.019014    -1.011674  ...    -0.057666         -0.072358   \n",
      "2109  0.151435 -0.117963    -1.011674  ...    -0.057666         -0.072358   \n",
      "2110  0.018992  0.092410    -1.011674  ...    -0.057666         -0.072358   \n",
      "\n",
      "      MTRANS_Public_Transportation  MTRANS_Walking  NObeyesdad_Normal_Weight  \\\n",
      "0                         0.579583       -0.165038                  2.520395   \n",
      "1                         0.579583       -0.165038                  2.520395   \n",
      "2                         0.579583       -0.165038                  2.520395   \n",
      "3                        -1.724560        6.056323                 -0.396575   \n",
      "4                         0.579583       -0.165038                 -0.396575   \n",
      "...                            ...             ...                       ...   \n",
      "2106                      0.579583       -0.165038                 -0.396575   \n",
      "2107                      0.579583       -0.165038                 -0.396575   \n",
      "2108                      0.579583       -0.165038                 -0.396575   \n",
      "2109                      0.579583       -0.165038                 -0.396575   \n",
      "2110                      0.579583       -0.165038                 -0.396575   \n",
      "\n",
      "      NObeyesdad_Obesity_Type_I  NObeyesdad_Obesity_Type_II  \\\n",
      "0                     -0.446472                   -0.404536   \n",
      "1                     -0.446472                   -0.404536   \n",
      "2                     -0.446472                   -0.404536   \n",
      "3                     -0.446472                   -0.404536   \n",
      "4                     -0.446472                   -0.404536   \n",
      "...                         ...                         ...   \n",
      "2106                  -0.446472                   -0.404536   \n",
      "2107                  -0.446472                   -0.404536   \n",
      "2108                  -0.446472                   -0.404536   \n",
      "2109                  -0.446472                   -0.404536   \n",
      "2110                  -0.446472                   -0.404536   \n",
      "\n",
      "      NObeyesdad_Obesity_Type_III  NObeyesdad_Overweight_Level_I  \\\n",
      "0                       -0.425704                      -0.398971   \n",
      "1                       -0.425704                      -0.398971   \n",
      "2                       -0.425704                      -0.398971   \n",
      "3                       -0.425704                       2.505262   \n",
      "4                       -0.425704                      -0.398971   \n",
      "...                           ...                            ...   \n",
      "2106                     2.347939                      -0.398971   \n",
      "2107                     2.347939                      -0.398971   \n",
      "2108                     2.347939                      -0.398971   \n",
      "2109                     2.347939                      -0.398971   \n",
      "2110                     2.347939                      -0.398971   \n",
      "\n",
      "      NObeyesdad_Overweight_Level_II  \n",
      "0                          -0.398971  \n",
      "1                          -0.398971  \n",
      "2                          -0.398971  \n",
      "3                          -0.398971  \n",
      "4                           2.505262  \n",
      "...                              ...  \n",
      "2106                       -0.398971  \n",
      "2107                       -0.398971  \n",
      "2108                       -0.398971  \n",
      "2109                       -0.398971  \n",
      "2110                       -0.398971  \n",
      "\n",
      "[2111 rows x 30 columns]\n"
     ]
    }
   ],
   "source": [
    "'''Downloading dataset'''\n",
    "dataset = pd.read_csv(\"/Users/alessandrococcia/Downloads/ObesityDataSet.csv\")\n",
    "\n",
    "'''Dataset sampling'''\n",
    "dataset.sample(frac=1, ignore_index=True) #shuffle sample in the training set\n",
    "\n",
    "'''mapping delle stringhe'''\n",
    "\n",
    "dataset = pd.get_dummies(dataset, drop_first=True, dtype=float)\n",
    "\n",
    "'''normalization'''\n",
    "#dataset = (dataset-dataset.min())/(dataset.max()-dataset.min())\n",
    "dataset =(dataset-dataset.mean())/dataset.std()\n",
    "\n",
    "'''Inserimento colonna di bias'''\n",
    "dataset.insert(0, \"Bias\", np.ones(len(dataset)), True) #Bias row\n",
    "\n",
    "TRAIN_TEST_SPLIT_PERCENTAGE = 0.9\n",
    "dataset_training = dataset[:int(len(dataset) * TRAIN_TEST_SPLIT_PERCENTAGE)]\n",
    "dataset_test = dataset[int(len(dataset) * TRAIN_TEST_SPLIT_PERCENTAGE):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b32a522f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Map:\n",
    "    def __init__(self):\n",
    "        self.count = 0\n",
    "        self.count_dataset = {}\n",
    "        self.d = {}\n",
    "        self.dataset_dictionary = {}\n",
    "        self.matrice = None\n",
    "        self.dataset = None\n",
    "        len = None\n",
    "    \n",
    "    def mappingElement(self, string):\n",
    "        if type(string) is str:\n",
    "            if self.d.get(string) is not None:\n",
    "                return self.d.get(string)\n",
    "            else:\n",
    "                self.count = self.count+1\n",
    "                self.d[string] = self.count\n",
    "                return self.count\n",
    "        return float(string)\n",
    "    \n",
    "    def mappingMatrix(self, matrice):\n",
    "        self.matrice = None\n",
    "        self.matrice = np.array(matrice)\n",
    "        len = self.matrice.shape\n",
    "        for g in range(len[0]):\n",
    "            for j in range(len[1]):\n",
    "                self.matrice[g][j] = self.mappingElement(self.matrice[g][j])\n",
    "        return np.float64(self.matrice.copy())\n",
    "    \n",
    "    def mappingElementDataset(self, value, colonna):\n",
    "        if type(value) is str:\n",
    "            if self.dataset_dictionary[colonna].get(value) is not None:\n",
    "                return self.dataset_dictionary[colonna].get(value)\n",
    "            else:\n",
    "                if self.count_dataset.get(colonna) is None:\n",
    "                    self.count_dataset[colonna] = 0\n",
    "                    self.dataset_dictionary[colonna][value] = self.count_dataset[colonna]\n",
    "                    return self.count_dataset[colonna]\n",
    "                self.count_dataset[colonna] += 1\n",
    "                self.dataset_dictionary[colonna][value] = self.count_dataset[colonna]\n",
    "                return self.count_dataset[colonna]\n",
    "        return value\n",
    "    \n",
    "    def mappingDataset(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        colonne = dataset.columns\n",
    "        length = len(dataset)\n",
    "        for column in colonne:\n",
    "            self.dataset_dictionary[column] = {}\n",
    "            for index in range(length):\n",
    "                dataset.at[index, column] = self.mappingElementDataset(dataset.at[index, column], column)\n",
    "        return self.dataset\n",
    "\n",
    "    def MSE(self, a, b):\n",
    "        return np.mean((np.square(a - b)))\n",
    "    def RMSE(self, a, b):\n",
    "        return math.sqrt(np.mean(np.square(a - b)))\n",
    "    def MAE(self, a, b):\n",
    "        return np.mean(abs(a-b))\n",
    "    \n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, outputstring, positive, negative): \n",
    "        self.outputstring = outputstring\n",
    "        self.positive = positive\n",
    "        self.negative = negative\n",
    "        self.nodes = []\n",
    "        self.label = None\n",
    "        self.root = False\n",
    "\n",
    "    def LearnDecisionTree(self, examples, attributes, parent_examples, column_values):\n",
    "        self.root = True\n",
    "        self.LearnDecisionTreeAux_(examples, attributes, parent_examples, column_values)\n",
    "\n",
    "    def LearnDecisionTreeAux_(self, examples, attributes, parent_examples, column_values):\n",
    "        same_classification = self.SameClassification(examples)    \n",
    "        \n",
    "        if len(examples.loc[:, examples.columns != self.outputstring]) == 0:\n",
    "            return self.PluralityValue(parent_examples)\n",
    "        elif same_classification is not False:\n",
    "            return same_classification\n",
    "        elif len(attributes) == 0: \n",
    "            return self.PluralityValue(examples)\n",
    "        else:\n",
    "            '''Seleziono l'attributo migliore'''\n",
    "            bestattribute = self.Importance(attributes, examples)\n",
    "            self.label = bestattribute #Seleziono l'attributo migliore\n",
    "            \n",
    "            for value in self.Values1(bestattribute, column_values):\n",
    "                remainingexamples = self.Examples(bestattribute, examples, value)\n",
    "                tree = DecisionTree(self.outputstring, self.positive, self.negative)\n",
    "                attributes_left = self.PopListValue(attributes.copy(), bestattribute)                \n",
    "                subtree = tree.LearnDecisionTreeAux_(remainingexamples.loc[:, remainingexamples.columns != bestattribute], attributes_left, examples, column_values)\n",
    "                self.nodes.append((value, subtree))\n",
    "        return self\n",
    "        \n",
    "    def Importance(self, attributes, examples): \n",
    "        max = -1\n",
    "        ret = None\n",
    "        for a in attributes:\n",
    "            loc = self.Gain(examples, a)\n",
    "            if loc > max:\n",
    "                max = loc\n",
    "                ret = a\n",
    "        return ret\n",
    "        \n",
    "    def B(self, q):\n",
    "        if q == 1 or q == 0:\n",
    "            return 0\n",
    "        return -(q*math.log2(q)+(1-q)*math.log2(1-q))\n",
    "    \n",
    "    def Remainder(self, examples, attribute, p, n):\n",
    "        sum = 0\n",
    "        for v in self.Values2(attribute, examples):\n",
    "            if type(v) is not str and math.isnan(float(v)):\n",
    "                pk = len(examples.loc[(examples[self.outputstring] == self.positive) & (examples[attribute].isnull())])\n",
    "                nk = len(examples.loc[(examples[self.outputstring] == self.negative) & (examples[attribute].isnull())])\n",
    "            else:\n",
    "                pk = len(examples.loc[(examples[self.outputstring] == self.positive) & (examples[attribute] == v)])\n",
    "                nk = len(examples.loc[(examples[self.outputstring] == self.negative) & ((examples[attribute] == v))])\n",
    "            \n",
    "            partial = (pk+nk)*self.B(pk/(pk+nk))\n",
    "            sum += partial\n",
    "        return (1/(p+n))*sum\n",
    "    \n",
    "    def Gain(self, examples, attribute):\n",
    "        p = len(examples.loc[examples[self.outputstring] == self.positive])\n",
    "        n = len(examples.loc[examples[self.outputstring] == self.negative])\n",
    "        b = self.B(p/(p+n))\n",
    "        r = self.Remainder(examples, attribute, p, n)\n",
    "        return (b-r)\n",
    "        \n",
    "    def PluralityValue(self, parent_examples): \n",
    "        '''Selects the most common ouput value among a set of examples, breaking ties randomly'''\n",
    "        value, max = [], 0\n",
    "        d = self.CreateDictionary(parent_examples)\n",
    "        for key in d.keys():\n",
    "            if d.get(key) > max:\n",
    "                max = d.get(key)\n",
    "                value = []\n",
    "                value.append(key)\n",
    "            elif d.get(key) == max:\n",
    "                '''se ci sono più massimi li metto tutti in una lista dalla quale ne\n",
    "                    selezionerò uno randomicamente\n",
    "                '''\n",
    "                value.append(key)\n",
    "        return rn.choice(value)\n",
    "        \n",
    "    def SameClassification(self, examples): \n",
    "        if (len(examples) == 0):\n",
    "            return False\n",
    "        d = self.CreateDictionary(examples)\n",
    "        if (len(d.keys()) == 1):\n",
    "            return list(d.keys())[0]\n",
    "        return False\n",
    "    def CreateDictionary(self, examples):\n",
    "        examples = examples[self.outputstring].tolist()\n",
    "        d = {}\n",
    "        for i in range(len(examples)):\n",
    "            if d.get(examples[i]) is None:\n",
    "                d[examples[i]] = 1\n",
    "            else:\n",
    "                d[examples[i]] += 1\n",
    "        return d\n",
    "\n",
    "    def Values1(self, attribute, dictionary): \n",
    "        return list(dictionary.get(attribute))\n",
    "    \n",
    "    def Values2(self, attribute, examples):\n",
    "        return examples[attribute].unique()\n",
    "\n",
    "    def Examples(self, attribute, examples, value):\n",
    "        exs = examples.loc[examples[attribute] == value]\n",
    "        return exs\n",
    "    def PopListValue(self, lista, value):\n",
    "        if value not in lista:\n",
    "            return None\n",
    "        lista.remove(value)\n",
    "        return lista \n",
    "    def PrintDecisionTree(self, count):\n",
    "        print(\" \"*count, self.label)\n",
    "        for elem in self.nodes:\n",
    "            if type(elem[1]) is not DecisionTree:\n",
    "                print(\"    \"*(count+1), elem[0], \" --> \", elem[1])\n",
    "            else:\n",
    "                elem[1].PrintDecisionTree(count+1)\n",
    "\n",
    "    def Prediction(self, input):\n",
    "        for i in range(len(self.nodes)):\n",
    "            x = input[self.label].values[0]\n",
    "            if type(self.nodes[i][0]) is not str and math.isnan(float(self.nodes[i][0])):\n",
    "                if type(x) is not str and math.isnan(float(x)):\n",
    "                    if type(self.nodes[i][1]) is DecisionTree:\n",
    "                        return self.nodes[i][1].Prediction(input)\n",
    "                    return self.nodes[i][1]\n",
    "            elif self.nodes[i][0] == x:\n",
    "                if type(self.nodes[i][1]) is DecisionTree:\n",
    "                    return self.nodes[i][1].Prediction(input)\n",
    "                return self.nodes[i][1]\n",
    "        return None\n",
    "\n",
    "class GradientDescent:\n",
    "    def __init__(self, learningrate, epochs, parameters_length, output_string, batch_size): \n",
    "        self.learningrate = learningrate\n",
    "        self.epochs = epochs\n",
    "        self.parameters_length = parameters_length\n",
    "        self.parameters = None\n",
    "        self.output_string = output_string\n",
    "        self.len_totale = None\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def SGD(self, training_set):\n",
    "        \n",
    "        '''Inizializzo i pesi ad 1'''\n",
    "        self.parameters = np.ones(self.parameters_length)\n",
    "        '''Su input e output totale calcolerò ad ogni n-esima epoca l'errore'''\n",
    "        input_totale = training_set.loc[:len(training_set), training_set.columns != self.output_string]\n",
    "        output_totale = training_set.loc[:len(training_set), training_set.columns == self.output_string]\n",
    "        self.len_totale = len(training_set)\n",
    "\n",
    "        '''ciclo for per le epoche'''\n",
    "        '''Batch == input totale: non devo randomizzare'''\n",
    "        if self.batch_size == self.len_totale:\n",
    "            input = input_totale\n",
    "            output = output_totale\n",
    "            for i in range(self.epochs):\n",
    "                gradient = self.gradient(input, output)\n",
    "                self.SGD_aux(gradient)\n",
    "                if (not (i%10000)):\n",
    "                    print(\"Epoca: \\t\", i)\n",
    "                    print(\"MSE: \", self.MSE(self.prediction(input_totale), output_totale))\n",
    "        \n",
    "        else:\n",
    "            '''uso delle minibatch che devono avere i sample randomizzati ad ogni iterazione'''\n",
    "            for i in range(self.epochs):\n",
    "                batch = training_set.sample(n=self.batch_size)\n",
    "                input = batch.loc[:, training_set.columns != self.output_string]\n",
    "                output = batch.loc[:, training_set.columns == self.output_string]\n",
    "                gradient = self.gradient(input, output)\n",
    "                self.SGD_aux(gradient)\n",
    "                \n",
    "                if (not (i%10000)):\n",
    "                    print(\"Epoca: \\t\", i)\n",
    "                    print(\"MSE: \", self.MSE(self.prediction(input_totale), output_totale))\n",
    "                \n",
    "        return self.parameters\n",
    "\n",
    "    def SGD_aux(self, gradient):\n",
    "        self.parameters -= self.learningrate*gradient\n",
    "\n",
    "    def gradient(self, input, output): \n",
    "        output = np.array(output).T\n",
    "        prediction = self.prediction(input)\n",
    "        error = prediction - output\n",
    "        return np.matmul(error, input)\n",
    "\n",
    "    def prediction(self, input):\n",
    "        return np.dot(input, self.parameters)\n",
    "\n",
    "    def MSE(self, prediction, output):\n",
    "        output = np.array(output)\n",
    "        return np.mean((output - prediction)**2)\n",
    "    def RMSE(self, prediction, output):\n",
    "        return math.sqrt(np.mean((output - prediction)**2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2fa37c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoca: \t 0\n",
      "MSE:  13.39974846789776\n",
      "Epoca: \t 10000\n",
      "MSE:  1.700445739078476\n",
      "Epoca: \t 20000\n",
      "MSE:  1.7003836509892576\n",
      "Epoca: \t 30000\n",
      "MSE:  1.7003805399196217\n",
      "Epoca: \t 40000\n",
      "MSE:  1.7003802232550707\n",
      "Epoca: \t 50000\n",
      "MSE:  1.7003801846881044\n",
      "Epoca: \t 60000\n",
      "MSE:  1.700380179868112\n",
      "Epoca: \t 70000\n",
      "MSE:  1.7003801792637336\n",
      "Epoca: \t 80000\n",
      "MSE:  1.7003801791879185\n",
      "Epoca: \t 90000\n",
      "MSE:  1.700380179178409\n",
      "123.0111870765686 [ 2.36090422e-03 -9.79275196e-03  3.22704698e-01  5.12680295e-03\n",
      " -1.47954411e-02  1.30563210e-02  1.99895760e-03  6.13404878e-03\n",
      "  1.17345052e-02 -7.86753317e-03 -5.80704498e-03  2.13760840e-02\n",
      " -6.32814880e-03 -5.57602021e-05  2.79603321e-03 -1.19273876e-03\n",
      "  9.80289315e-03  4.38575887e-02  2.46636951e-02 -4.28506261e-03\n",
      " -5.19387392e-03 -1.21828187e-02 -3.60840145e-03  1.81521262e-01\n",
      "  6.23718119e-01  7.84633053e-01  1.00463524e+00  3.32728495e-01\n",
      "  4.20674500e-01]\n"
     ]
    }
   ],
   "source": [
    "gd = GradientDescent(1e-4, 100000, dataset_training.shape[1]-1, 'Weight', dataset_training.shape[0])\n",
    "\n",
    "start = time.time()\n",
    "pesi = gd.SGD(dataset_training)\n",
    "end = time.time()\n",
    "print(end-start, pesi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "21aa53e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  0.13399843847999332\n",
      "RMSE:  0.36605797147445557\n",
      "MAE:  0.32018768237759465\n",
      "MSE_sklearn:  0.377664398692556\n",
      "RMSE_sklearn:  0.6145440575683374\n",
      "MAE_sklearn:  0.5249601168214079\n",
      "MSE_sklearn SGD:  0.14643371987563464\n",
      "RMSE_sklearn SGD:  0.38266659101054884\n",
      "MAE_sklearn SGD:  0.3353679576421768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "X_test = np.array(dataset_test.loc[:, dataset_test.columns != 'Weight'])\n",
    "Y_effettivo = np.array(dataset_test['Weight'])\n",
    "Y_predetto = np.array(np.matmul(X_test, pesi))\n",
    "\n",
    "\n",
    "\n",
    "map = Map()\n",
    "print(\"MSE: \", map.MSE(Y_effettivo, Y_predetto))\n",
    "print(\"RMSE: \", map.RMSE(Y_effettivo, Y_predetto))\n",
    "print(\"MAE: \", map.MAE(Y_effettivo, Y_predetto))\n",
    "\n",
    "'''sklearn'''\n",
    "\n",
    "X = np.array(dataset_training.loc[:, dataset_training.columns != 'Weight'])\n",
    "Y = np.array(dataset_training.loc[:, dataset_training.columns == 'Weight'])\n",
    "\n",
    "reg = LinearRegression().fit(X, Y)\n",
    "scikit_predict = np.array(reg.predict(X_test))\n",
    "\n",
    "print(\"MSE_sklearn: \", map.MSE(Y_effettivo, scikit_predict))\n",
    "print(\"RMSE_sklearn: \", map.RMSE(Y_effettivo, scikit_predict))\n",
    "print(\"MAE_sklearn: \", map.MAE(Y_effettivo, scikit_predict))\n",
    "\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "# X_test e Y_effettivo sono i dati di test\n",
    "\n",
    "# Definisci il regressore SGD\n",
    "sgd_regressor = SGDRegressor(max_iter=100000000, alpha=0.001, random_state=42)\n",
    "\n",
    "# Addestra il modello\n",
    "sgd_regressor.fit(X, Y)\n",
    "\n",
    "# Effettua previsioni\n",
    "Y_predetto_sgd = np.array(sgd_regressor.predict(X_test))\n",
    "\n",
    "# Calcola le metriche di valutazione\n",
    "\n",
    "# Stampare le metriche di valutazione\n",
    "print(\"MSE_sklearn SGD: \", map.MSE(Y_effettivo, Y_predetto_sgd))\n",
    "print(\"RMSE_sklearn SGD: \", map.RMSE(Y_effettivo, Y_predetto_sgd))\n",
    "print(\"MAE_sklearn SGD: \", map.MAE(Y_effettivo, Y_predetto_sgd))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
